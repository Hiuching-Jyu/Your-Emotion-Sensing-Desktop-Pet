{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1b837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Start TinySimCLR training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|████████████████████████████████████| 669/669 [05:44<00:00,  1.94it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished | avg loss: 0.0044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|████████████████████████████████████| 669/669 [05:41<00:00,  1.96it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished | avg loss: 0.0002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|████████████████████████████████████| 669/669 [05:48<00:00,  1.92it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished | avg loss: 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|████████████████████████████████████| 669/669 [05:47<00:00,  1.93it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|████████████████████████████████████| 669/669 [05:42<00:00,  1.95it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|████████████████████████████████████| 669/669 [05:43<00:00,  1.95it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|████████████████████████████████████| 669/669 [05:46<00:00,  1.93it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|████████████████████████████████████| 669/669 [05:41<00:00,  1.96it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|████████████████████████████████████| 669/669 [05:45<00:00,  1.93it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|███████████████████████████████████| 669/669 [05:46<00:00,  1.93it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|███████████████████████████████████| 669/669 [05:50<00:00,  1.91it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|███████████████████████████████████| 669/669 [05:47<00:00,  1.92it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|███████████████████████████████████| 669/669 [05:49<00:00,  1.91it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|███████████████████████████████████| 669/669 [05:45<00:00,  1.94it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished | avg loss: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|███████████████████████████████████| 669/669 [05:45<00:00,  1.94it/s, loss=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished | avg loss: 0.0000\n",
      "\n",
      "Training complete! Saved as tinysimclr_effb0_mac.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Data augmentation (SimCLR required; Tiny version offers lighter augmentation)\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 2. Loading the dataset\n",
    "dataset = torchvision.datasets.ImageFolder(\"YOLO_format_cls/train\", transform=None)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 3. Backbone + Projection head（EfficientNet-B0）\n",
    "class TinySimCLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "        base.classifier = nn.Identity()  # Remove classification layer\n",
    "        self.encoder = base\n",
    "\n",
    "        # Tiny Projection Head (Compact dimensions, minimal graphics memory)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return nn.functional.normalize(z, dim=1)\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = TinySimCLR().to(device)\n",
    "\n",
    "# 4. Tiny Contrastive Loss Function (No NxN Similarity Matrix Required)\n",
    "def tiny_contrastive_loss(z1, z2):\n",
    "    # cosine similarity (negative sign becomes distance)\n",
    "    return 1 - torch.mean(torch.sum(z1 * z2, dim=1))\n",
    "\n",
    "# 5. Training Cycle\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "print(\"Start TinySimCLR training...\\n\")\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "    for imgs, _ in pbar:\n",
    "        # Two randomly enhanced versions\n",
    "        x1 = torch.stack([simclr_transform(img) for img in imgs]).to(device)\n",
    "        x2 = torch.stack([simclr_transform(img) for img in imgs]).to(device)\n",
    "\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "\n",
    "        loss = tiny_contrastive_loss(z1, z2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished | avg loss: {epoch_loss / len(loader):.4f}\\n\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"tinysimclr_effb0_mac.pth\")\n",
    "print(\"Training complete! Saved as tinysimclr_effb0_mac.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10714, Val size: 3129\n",
      "Loaded SimCLR backbone. Missing keys: ['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var', 'features.1.0.block.1.fc1.weight', 'features.1.0.block.1.fc1.bias', 'features.1.0.block.1.fc2.weight', 'features.1.0.block.1.fc2.bias', 'features.1.0.block.2.0.weight', 'features.1.0.block.2.1.weight', 'features.1.0.block.2.1.bias', 'features.1.0.block.2.1.running_mean', 'features.1.0.block.2.1.running_var', 'features.2.0.block.0.0.weight', 'features.2.0.block.0.1.weight', 'features.2.0.block.0.1.bias', 'features.2.0.block.0.1.running_mean', 'features.2.0.block.0.1.running_var', 'features.2.0.block.1.0.weight', 'features.2.0.block.1.1.weight', 'features.2.0.block.1.1.bias', 'features.2.0.block.1.1.running_mean', 'features.2.0.block.1.1.running_var', 'features.2.0.block.2.fc1.weight', 'features.2.0.block.2.fc1.bias', 'features.2.0.block.2.fc2.weight', 'features.2.0.block.2.fc2.bias', 'features.2.0.block.3.0.weight', 'features.2.0.block.3.1.weight', 'features.2.0.block.3.1.bias', 'features.2.0.block.3.1.running_mean', 'features.2.0.block.3.1.running_var', 'features.2.1.block.0.0.weight', 'features.2.1.block.0.1.weight', 'features.2.1.block.0.1.bias', 'features.2.1.block.0.1.running_mean', 'features.2.1.block.0.1.running_var', 'features.2.1.block.1.0.weight', 'features.2.1.block.1.1.weight', 'features.2.1.block.1.1.bias', 'features.2.1.block.1.1.running_mean', 'features.2.1.block.1.1.running_var', 'features.2.1.block.2.fc1.weight', 'features.2.1.block.2.fc1.bias', 'features.2.1.block.2.fc2.weight', 'features.2.1.block.2.fc2.bias', 'features.2.1.block.3.0.weight', 'features.2.1.block.3.1.weight', 'features.2.1.block.3.1.bias', 'features.2.1.block.3.1.running_mean', 'features.2.1.block.3.1.running_var', 'features.3.0.block.0.0.weight', 'features.3.0.block.0.1.weight', 'features.3.0.block.0.1.bias', 'features.3.0.block.0.1.running_mean', 'features.3.0.block.0.1.running_var', 'features.3.0.block.1.0.weight', 'features.3.0.block.1.1.weight', 'features.3.0.block.1.1.bias', 'features.3.0.block.1.1.running_mean', 'features.3.0.block.1.1.running_var', 'features.3.0.block.2.fc1.weight', 'features.3.0.block.2.fc1.bias', 'features.3.0.block.2.fc2.weight', 'features.3.0.block.2.fc2.bias', 'features.3.0.block.3.0.weight', 'features.3.0.block.3.1.weight', 'features.3.0.block.3.1.bias', 'features.3.0.block.3.1.running_mean', 'features.3.0.block.3.1.running_var', 'features.3.1.block.0.0.weight', 'features.3.1.block.0.1.weight', 'features.3.1.block.0.1.bias', 'features.3.1.block.0.1.running_mean', 'features.3.1.block.0.1.running_var', 'features.3.1.block.1.0.weight', 'features.3.1.block.1.1.weight', 'features.3.1.block.1.1.bias', 'features.3.1.block.1.1.running_mean', 'features.3.1.block.1.1.running_var', 'features.3.1.block.2.fc1.weight', 'features.3.1.block.2.fc1.bias', 'features.3.1.block.2.fc2.weight', 'features.3.1.block.2.fc2.bias', 'features.3.1.block.3.0.weight', 'features.3.1.block.3.1.weight', 'features.3.1.block.3.1.bias', 'features.3.1.block.3.1.running_mean', 'features.3.1.block.3.1.running_var', 'features.4.0.block.0.0.weight', 'features.4.0.block.0.1.weight', 'features.4.0.block.0.1.bias', 'features.4.0.block.0.1.running_mean', 'features.4.0.block.0.1.running_var', 'features.4.0.block.1.0.weight', 'features.4.0.block.1.1.weight', 'features.4.0.block.1.1.bias', 'features.4.0.block.1.1.running_mean', 'features.4.0.block.1.1.running_var', 'features.4.0.block.2.fc1.weight', 'features.4.0.block.2.fc1.bias', 'features.4.0.block.2.fc2.weight', 'features.4.0.block.2.fc2.bias', 'features.4.0.block.3.0.weight', 'features.4.0.block.3.1.weight', 'features.4.0.block.3.1.bias', 'features.4.0.block.3.1.running_mean', 'features.4.0.block.3.1.running_var', 'features.4.1.block.0.0.weight', 'features.4.1.block.0.1.weight', 'features.4.1.block.0.1.bias', 'features.4.1.block.0.1.running_mean', 'features.4.1.block.0.1.running_var', 'features.4.1.block.1.0.weight', 'features.4.1.block.1.1.weight', 'features.4.1.block.1.1.bias', 'features.4.1.block.1.1.running_mean', 'features.4.1.block.1.1.running_var', 'features.4.1.block.2.fc1.weight', 'features.4.1.block.2.fc1.bias', 'features.4.1.block.2.fc2.weight', 'features.4.1.block.2.fc2.bias', 'features.4.1.block.3.0.weight', 'features.4.1.block.3.1.weight', 'features.4.1.block.3.1.bias', 'features.4.1.block.3.1.running_mean', 'features.4.1.block.3.1.running_var', 'features.4.2.block.0.0.weight', 'features.4.2.block.0.1.weight', 'features.4.2.block.0.1.bias', 'features.4.2.block.0.1.running_mean', 'features.4.2.block.0.1.running_var', 'features.4.2.block.1.0.weight', 'features.4.2.block.1.1.weight', 'features.4.2.block.1.1.bias', 'features.4.2.block.1.1.running_mean', 'features.4.2.block.1.1.running_var', 'features.4.2.block.2.fc1.weight', 'features.4.2.block.2.fc1.bias', 'features.4.2.block.2.fc2.weight', 'features.4.2.block.2.fc2.bias', 'features.4.2.block.3.0.weight', 'features.4.2.block.3.1.weight', 'features.4.2.block.3.1.bias', 'features.4.2.block.3.1.running_mean', 'features.4.2.block.3.1.running_var', 'features.5.0.block.0.0.weight', 'features.5.0.block.0.1.weight', 'features.5.0.block.0.1.bias', 'features.5.0.block.0.1.running_mean', 'features.5.0.block.0.1.running_var', 'features.5.0.block.1.0.weight', 'features.5.0.block.1.1.weight', 'features.5.0.block.1.1.bias', 'features.5.0.block.1.1.running_mean', 'features.5.0.block.1.1.running_var', 'features.5.0.block.2.fc1.weight', 'features.5.0.block.2.fc1.bias', 'features.5.0.block.2.fc2.weight', 'features.5.0.block.2.fc2.bias', 'features.5.0.block.3.0.weight', 'features.5.0.block.3.1.weight', 'features.5.0.block.3.1.bias', 'features.5.0.block.3.1.running_mean', 'features.5.0.block.3.1.running_var', 'features.5.1.block.0.0.weight', 'features.5.1.block.0.1.weight', 'features.5.1.block.0.1.bias', 'features.5.1.block.0.1.running_mean', 'features.5.1.block.0.1.running_var', 'features.5.1.block.1.0.weight', 'features.5.1.block.1.1.weight', 'features.5.1.block.1.1.bias', 'features.5.1.block.1.1.running_mean', 'features.5.1.block.1.1.running_var', 'features.5.1.block.2.fc1.weight', 'features.5.1.block.2.fc1.bias', 'features.5.1.block.2.fc2.weight', 'features.5.1.block.2.fc2.bias', 'features.5.1.block.3.0.weight', 'features.5.1.block.3.1.weight', 'features.5.1.block.3.1.bias', 'features.5.1.block.3.1.running_mean', 'features.5.1.block.3.1.running_var', 'features.5.2.block.0.0.weight', 'features.5.2.block.0.1.weight', 'features.5.2.block.0.1.bias', 'features.5.2.block.0.1.running_mean', 'features.5.2.block.0.1.running_var', 'features.5.2.block.1.0.weight', 'features.5.2.block.1.1.weight', 'features.5.2.block.1.1.bias', 'features.5.2.block.1.1.running_mean', 'features.5.2.block.1.1.running_var', 'features.5.2.block.2.fc1.weight', 'features.5.2.block.2.fc1.bias', 'features.5.2.block.2.fc2.weight', 'features.5.2.block.2.fc2.bias', 'features.5.2.block.3.0.weight', 'features.5.2.block.3.1.weight', 'features.5.2.block.3.1.bias', 'features.5.2.block.3.1.running_mean', 'features.5.2.block.3.1.running_var', 'features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.0.1.running_mean', 'features.6.0.block.0.1.running_var', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.1.1.running_mean', 'features.6.0.block.1.1.running_var', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.0.block.3.1.running_mean', 'features.6.0.block.3.1.running_var', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.0.1.running_mean', 'features.6.1.block.0.1.running_var', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.1.1.running_mean', 'features.6.1.block.1.1.running_var', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.1.block.3.1.running_mean', 'features.6.1.block.3.1.running_var', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.0.1.running_mean', 'features.6.2.block.0.1.running_var', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.1.1.running_mean', 'features.6.2.block.1.1.running_var', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.2.block.3.1.running_mean', 'features.6.2.block.3.1.running_var', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.0.1.running_mean', 'features.6.3.block.0.1.running_var', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.1.1.running_mean', 'features.6.3.block.1.1.running_var', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.3.block.3.1.running_mean', 'features.6.3.block.3.1.running_var', 'features.7.0.block.0.0.weight', 'features.7.0.block.0.1.weight', 'features.7.0.block.0.1.bias', 'features.7.0.block.0.1.running_mean', 'features.7.0.block.0.1.running_var', 'features.7.0.block.1.0.weight', 'features.7.0.block.1.1.weight', 'features.7.0.block.1.1.bias', 'features.7.0.block.1.1.running_mean', 'features.7.0.block.1.1.running_var', 'features.7.0.block.2.fc1.weight', 'features.7.0.block.2.fc1.bias', 'features.7.0.block.2.fc2.weight', 'features.7.0.block.2.fc2.bias', 'features.7.0.block.3.0.weight', 'features.7.0.block.3.1.weight', 'features.7.0.block.3.1.bias', 'features.7.0.block.3.1.running_mean', 'features.7.0.block.3.1.running_var', 'features.8.0.weight', 'features.8.1.weight', 'features.8.1.bias', 'features.8.1.running_mean', 'features.8.1.running_var']\n",
      "Unexpected keys: ['encoder.features.0.0.weight', 'encoder.features.0.1.weight', 'encoder.features.0.1.bias', 'encoder.features.0.1.running_mean', 'encoder.features.0.1.running_var', 'encoder.features.0.1.num_batches_tracked', 'encoder.features.1.0.block.0.0.weight', 'encoder.features.1.0.block.0.1.weight', 'encoder.features.1.0.block.0.1.bias', 'encoder.features.1.0.block.0.1.running_mean', 'encoder.features.1.0.block.0.1.running_var', 'encoder.features.1.0.block.0.1.num_batches_tracked', 'encoder.features.1.0.block.1.fc1.weight', 'encoder.features.1.0.block.1.fc1.bias', 'encoder.features.1.0.block.1.fc2.weight', 'encoder.features.1.0.block.1.fc2.bias', 'encoder.features.1.0.block.2.0.weight', 'encoder.features.1.0.block.2.1.weight', 'encoder.features.1.0.block.2.1.bias', 'encoder.features.1.0.block.2.1.running_mean', 'encoder.features.1.0.block.2.1.running_var', 'encoder.features.1.0.block.2.1.num_batches_tracked', 'encoder.features.2.0.block.0.0.weight', 'encoder.features.2.0.block.0.1.weight', 'encoder.features.2.0.block.0.1.bias', 'encoder.features.2.0.block.0.1.running_mean', 'encoder.features.2.0.block.0.1.running_var', 'encoder.features.2.0.block.0.1.num_batches_tracked', 'encoder.features.2.0.block.1.0.weight', 'encoder.features.2.0.block.1.1.weight', 'encoder.features.2.0.block.1.1.bias', 'encoder.features.2.0.block.1.1.running_mean', 'encoder.features.2.0.block.1.1.running_var', 'encoder.features.2.0.block.1.1.num_batches_tracked', 'encoder.features.2.0.block.2.fc1.weight', 'encoder.features.2.0.block.2.fc1.bias', 'encoder.features.2.0.block.2.fc2.weight', 'encoder.features.2.0.block.2.fc2.bias', 'encoder.features.2.0.block.3.0.weight', 'encoder.features.2.0.block.3.1.weight', 'encoder.features.2.0.block.3.1.bias', 'encoder.features.2.0.block.3.1.running_mean', 'encoder.features.2.0.block.3.1.running_var', 'encoder.features.2.0.block.3.1.num_batches_tracked', 'encoder.features.2.1.block.0.0.weight', 'encoder.features.2.1.block.0.1.weight', 'encoder.features.2.1.block.0.1.bias', 'encoder.features.2.1.block.0.1.running_mean', 'encoder.features.2.1.block.0.1.running_var', 'encoder.features.2.1.block.0.1.num_batches_tracked', 'encoder.features.2.1.block.1.0.weight', 'encoder.features.2.1.block.1.1.weight', 'encoder.features.2.1.block.1.1.bias', 'encoder.features.2.1.block.1.1.running_mean', 'encoder.features.2.1.block.1.1.running_var', 'encoder.features.2.1.block.1.1.num_batches_tracked', 'encoder.features.2.1.block.2.fc1.weight', 'encoder.features.2.1.block.2.fc1.bias', 'encoder.features.2.1.block.2.fc2.weight', 'encoder.features.2.1.block.2.fc2.bias', 'encoder.features.2.1.block.3.0.weight', 'encoder.features.2.1.block.3.1.weight', 'encoder.features.2.1.block.3.1.bias', 'encoder.features.2.1.block.3.1.running_mean', 'encoder.features.2.1.block.3.1.running_var', 'encoder.features.2.1.block.3.1.num_batches_tracked', 'encoder.features.3.0.block.0.0.weight', 'encoder.features.3.0.block.0.1.weight', 'encoder.features.3.0.block.0.1.bias', 'encoder.features.3.0.block.0.1.running_mean', 'encoder.features.3.0.block.0.1.running_var', 'encoder.features.3.0.block.0.1.num_batches_tracked', 'encoder.features.3.0.block.1.0.weight', 'encoder.features.3.0.block.1.1.weight', 'encoder.features.3.0.block.1.1.bias', 'encoder.features.3.0.block.1.1.running_mean', 'encoder.features.3.0.block.1.1.running_var', 'encoder.features.3.0.block.1.1.num_batches_tracked', 'encoder.features.3.0.block.2.fc1.weight', 'encoder.features.3.0.block.2.fc1.bias', 'encoder.features.3.0.block.2.fc2.weight', 'encoder.features.3.0.block.2.fc2.bias', 'encoder.features.3.0.block.3.0.weight', 'encoder.features.3.0.block.3.1.weight', 'encoder.features.3.0.block.3.1.bias', 'encoder.features.3.0.block.3.1.running_mean', 'encoder.features.3.0.block.3.1.running_var', 'encoder.features.3.0.block.3.1.num_batches_tracked', 'encoder.features.3.1.block.0.0.weight', 'encoder.features.3.1.block.0.1.weight', 'encoder.features.3.1.block.0.1.bias', 'encoder.features.3.1.block.0.1.running_mean', 'encoder.features.3.1.block.0.1.running_var', 'encoder.features.3.1.block.0.1.num_batches_tracked', 'encoder.features.3.1.block.1.0.weight', 'encoder.features.3.1.block.1.1.weight', 'encoder.features.3.1.block.1.1.bias', 'encoder.features.3.1.block.1.1.running_mean', 'encoder.features.3.1.block.1.1.running_var', 'encoder.features.3.1.block.1.1.num_batches_tracked', 'encoder.features.3.1.block.2.fc1.weight', 'encoder.features.3.1.block.2.fc1.bias', 'encoder.features.3.1.block.2.fc2.weight', 'encoder.features.3.1.block.2.fc2.bias', 'encoder.features.3.1.block.3.0.weight', 'encoder.features.3.1.block.3.1.weight', 'encoder.features.3.1.block.3.1.bias', 'encoder.features.3.1.block.3.1.running_mean', 'encoder.features.3.1.block.3.1.running_var', 'encoder.features.3.1.block.3.1.num_batches_tracked', 'encoder.features.4.0.block.0.0.weight', 'encoder.features.4.0.block.0.1.weight', 'encoder.features.4.0.block.0.1.bias', 'encoder.features.4.0.block.0.1.running_mean', 'encoder.features.4.0.block.0.1.running_var', 'encoder.features.4.0.block.0.1.num_batches_tracked', 'encoder.features.4.0.block.1.0.weight', 'encoder.features.4.0.block.1.1.weight', 'encoder.features.4.0.block.1.1.bias', 'encoder.features.4.0.block.1.1.running_mean', 'encoder.features.4.0.block.1.1.running_var', 'encoder.features.4.0.block.1.1.num_batches_tracked', 'encoder.features.4.0.block.2.fc1.weight', 'encoder.features.4.0.block.2.fc1.bias', 'encoder.features.4.0.block.2.fc2.weight', 'encoder.features.4.0.block.2.fc2.bias', 'encoder.features.4.0.block.3.0.weight', 'encoder.features.4.0.block.3.1.weight', 'encoder.features.4.0.block.3.1.bias', 'encoder.features.4.0.block.3.1.running_mean', 'encoder.features.4.0.block.3.1.running_var', 'encoder.features.4.0.block.3.1.num_batches_tracked', 'encoder.features.4.1.block.0.0.weight', 'encoder.features.4.1.block.0.1.weight', 'encoder.features.4.1.block.0.1.bias', 'encoder.features.4.1.block.0.1.running_mean', 'encoder.features.4.1.block.0.1.running_var', 'encoder.features.4.1.block.0.1.num_batches_tracked', 'encoder.features.4.1.block.1.0.weight', 'encoder.features.4.1.block.1.1.weight', 'encoder.features.4.1.block.1.1.bias', 'encoder.features.4.1.block.1.1.running_mean', 'encoder.features.4.1.block.1.1.running_var', 'encoder.features.4.1.block.1.1.num_batches_tracked', 'encoder.features.4.1.block.2.fc1.weight', 'encoder.features.4.1.block.2.fc1.bias', 'encoder.features.4.1.block.2.fc2.weight', 'encoder.features.4.1.block.2.fc2.bias', 'encoder.features.4.1.block.3.0.weight', 'encoder.features.4.1.block.3.1.weight', 'encoder.features.4.1.block.3.1.bias', 'encoder.features.4.1.block.3.1.running_mean', 'encoder.features.4.1.block.3.1.running_var', 'encoder.features.4.1.block.3.1.num_batches_tracked', 'encoder.features.4.2.block.0.0.weight', 'encoder.features.4.2.block.0.1.weight', 'encoder.features.4.2.block.0.1.bias', 'encoder.features.4.2.block.0.1.running_mean', 'encoder.features.4.2.block.0.1.running_var', 'encoder.features.4.2.block.0.1.num_batches_tracked', 'encoder.features.4.2.block.1.0.weight', 'encoder.features.4.2.block.1.1.weight', 'encoder.features.4.2.block.1.1.bias', 'encoder.features.4.2.block.1.1.running_mean', 'encoder.features.4.2.block.1.1.running_var', 'encoder.features.4.2.block.1.1.num_batches_tracked', 'encoder.features.4.2.block.2.fc1.weight', 'encoder.features.4.2.block.2.fc1.bias', 'encoder.features.4.2.block.2.fc2.weight', 'encoder.features.4.2.block.2.fc2.bias', 'encoder.features.4.2.block.3.0.weight', 'encoder.features.4.2.block.3.1.weight', 'encoder.features.4.2.block.3.1.bias', 'encoder.features.4.2.block.3.1.running_mean', 'encoder.features.4.2.block.3.1.running_var', 'encoder.features.4.2.block.3.1.num_batches_tracked', 'encoder.features.5.0.block.0.0.weight', 'encoder.features.5.0.block.0.1.weight', 'encoder.features.5.0.block.0.1.bias', 'encoder.features.5.0.block.0.1.running_mean', 'encoder.features.5.0.block.0.1.running_var', 'encoder.features.5.0.block.0.1.num_batches_tracked', 'encoder.features.5.0.block.1.0.weight', 'encoder.features.5.0.block.1.1.weight', 'encoder.features.5.0.block.1.1.bias', 'encoder.features.5.0.block.1.1.running_mean', 'encoder.features.5.0.block.1.1.running_var', 'encoder.features.5.0.block.1.1.num_batches_tracked', 'encoder.features.5.0.block.2.fc1.weight', 'encoder.features.5.0.block.2.fc1.bias', 'encoder.features.5.0.block.2.fc2.weight', 'encoder.features.5.0.block.2.fc2.bias', 'encoder.features.5.0.block.3.0.weight', 'encoder.features.5.0.block.3.1.weight', 'encoder.features.5.0.block.3.1.bias', 'encoder.features.5.0.block.3.1.running_mean', 'encoder.features.5.0.block.3.1.running_var', 'encoder.features.5.0.block.3.1.num_batches_tracked', 'encoder.features.5.1.block.0.0.weight', 'encoder.features.5.1.block.0.1.weight', 'encoder.features.5.1.block.0.1.bias', 'encoder.features.5.1.block.0.1.running_mean', 'encoder.features.5.1.block.0.1.running_var', 'encoder.features.5.1.block.0.1.num_batches_tracked', 'encoder.features.5.1.block.1.0.weight', 'encoder.features.5.1.block.1.1.weight', 'encoder.features.5.1.block.1.1.bias', 'encoder.features.5.1.block.1.1.running_mean', 'encoder.features.5.1.block.1.1.running_var', 'encoder.features.5.1.block.1.1.num_batches_tracked', 'encoder.features.5.1.block.2.fc1.weight', 'encoder.features.5.1.block.2.fc1.bias', 'encoder.features.5.1.block.2.fc2.weight', 'encoder.features.5.1.block.2.fc2.bias', 'encoder.features.5.1.block.3.0.weight', 'encoder.features.5.1.block.3.1.weight', 'encoder.features.5.1.block.3.1.bias', 'encoder.features.5.1.block.3.1.running_mean', 'encoder.features.5.1.block.3.1.running_var', 'encoder.features.5.1.block.3.1.num_batches_tracked', 'encoder.features.5.2.block.0.0.weight', 'encoder.features.5.2.block.0.1.weight', 'encoder.features.5.2.block.0.1.bias', 'encoder.features.5.2.block.0.1.running_mean', 'encoder.features.5.2.block.0.1.running_var', 'encoder.features.5.2.block.0.1.num_batches_tracked', 'encoder.features.5.2.block.1.0.weight', 'encoder.features.5.2.block.1.1.weight', 'encoder.features.5.2.block.1.1.bias', 'encoder.features.5.2.block.1.1.running_mean', 'encoder.features.5.2.block.1.1.running_var', 'encoder.features.5.2.block.1.1.num_batches_tracked', 'encoder.features.5.2.block.2.fc1.weight', 'encoder.features.5.2.block.2.fc1.bias', 'encoder.features.5.2.block.2.fc2.weight', 'encoder.features.5.2.block.2.fc2.bias', 'encoder.features.5.2.block.3.0.weight', 'encoder.features.5.2.block.3.1.weight', 'encoder.features.5.2.block.3.1.bias', 'encoder.features.5.2.block.3.1.running_mean', 'encoder.features.5.2.block.3.1.running_var', 'encoder.features.5.2.block.3.1.num_batches_tracked', 'encoder.features.6.0.block.0.0.weight', 'encoder.features.6.0.block.0.1.weight', 'encoder.features.6.0.block.0.1.bias', 'encoder.features.6.0.block.0.1.running_mean', 'encoder.features.6.0.block.0.1.running_var', 'encoder.features.6.0.block.0.1.num_batches_tracked', 'encoder.features.6.0.block.1.0.weight', 'encoder.features.6.0.block.1.1.weight', 'encoder.features.6.0.block.1.1.bias', 'encoder.features.6.0.block.1.1.running_mean', 'encoder.features.6.0.block.1.1.running_var', 'encoder.features.6.0.block.1.1.num_batches_tracked', 'encoder.features.6.0.block.2.fc1.weight', 'encoder.features.6.0.block.2.fc1.bias', 'encoder.features.6.0.block.2.fc2.weight', 'encoder.features.6.0.block.2.fc2.bias', 'encoder.features.6.0.block.3.0.weight', 'encoder.features.6.0.block.3.1.weight', 'encoder.features.6.0.block.3.1.bias', 'encoder.features.6.0.block.3.1.running_mean', 'encoder.features.6.0.block.3.1.running_var', 'encoder.features.6.0.block.3.1.num_batches_tracked', 'encoder.features.6.1.block.0.0.weight', 'encoder.features.6.1.block.0.1.weight', 'encoder.features.6.1.block.0.1.bias', 'encoder.features.6.1.block.0.1.running_mean', 'encoder.features.6.1.block.0.1.running_var', 'encoder.features.6.1.block.0.1.num_batches_tracked', 'encoder.features.6.1.block.1.0.weight', 'encoder.features.6.1.block.1.1.weight', 'encoder.features.6.1.block.1.1.bias', 'encoder.features.6.1.block.1.1.running_mean', 'encoder.features.6.1.block.1.1.running_var', 'encoder.features.6.1.block.1.1.num_batches_tracked', 'encoder.features.6.1.block.2.fc1.weight', 'encoder.features.6.1.block.2.fc1.bias', 'encoder.features.6.1.block.2.fc2.weight', 'encoder.features.6.1.block.2.fc2.bias', 'encoder.features.6.1.block.3.0.weight', 'encoder.features.6.1.block.3.1.weight', 'encoder.features.6.1.block.3.1.bias', 'encoder.features.6.1.block.3.1.running_mean', 'encoder.features.6.1.block.3.1.running_var', 'encoder.features.6.1.block.3.1.num_batches_tracked', 'encoder.features.6.2.block.0.0.weight', 'encoder.features.6.2.block.0.1.weight', 'encoder.features.6.2.block.0.1.bias', 'encoder.features.6.2.block.0.1.running_mean', 'encoder.features.6.2.block.0.1.running_var', 'encoder.features.6.2.block.0.1.num_batches_tracked', 'encoder.features.6.2.block.1.0.weight', 'encoder.features.6.2.block.1.1.weight', 'encoder.features.6.2.block.1.1.bias', 'encoder.features.6.2.block.1.1.running_mean', 'encoder.features.6.2.block.1.1.running_var', 'encoder.features.6.2.block.1.1.num_batches_tracked', 'encoder.features.6.2.block.2.fc1.weight', 'encoder.features.6.2.block.2.fc1.bias', 'encoder.features.6.2.block.2.fc2.weight', 'encoder.features.6.2.block.2.fc2.bias', 'encoder.features.6.2.block.3.0.weight', 'encoder.features.6.2.block.3.1.weight', 'encoder.features.6.2.block.3.1.bias', 'encoder.features.6.2.block.3.1.running_mean', 'encoder.features.6.2.block.3.1.running_var', 'encoder.features.6.2.block.3.1.num_batches_tracked', 'encoder.features.6.3.block.0.0.weight', 'encoder.features.6.3.block.0.1.weight', 'encoder.features.6.3.block.0.1.bias', 'encoder.features.6.3.block.0.1.running_mean', 'encoder.features.6.3.block.0.1.running_var', 'encoder.features.6.3.block.0.1.num_batches_tracked', 'encoder.features.6.3.block.1.0.weight', 'encoder.features.6.3.block.1.1.weight', 'encoder.features.6.3.block.1.1.bias', 'encoder.features.6.3.block.1.1.running_mean', 'encoder.features.6.3.block.1.1.running_var', 'encoder.features.6.3.block.1.1.num_batches_tracked', 'encoder.features.6.3.block.2.fc1.weight', 'encoder.features.6.3.block.2.fc1.bias', 'encoder.features.6.3.block.2.fc2.weight', 'encoder.features.6.3.block.2.fc2.bias', 'encoder.features.6.3.block.3.0.weight', 'encoder.features.6.3.block.3.1.weight', 'encoder.features.6.3.block.3.1.bias', 'encoder.features.6.3.block.3.1.running_mean', 'encoder.features.6.3.block.3.1.running_var', 'encoder.features.6.3.block.3.1.num_batches_tracked', 'encoder.features.7.0.block.0.0.weight', 'encoder.features.7.0.block.0.1.weight', 'encoder.features.7.0.block.0.1.bias', 'encoder.features.7.0.block.0.1.running_mean', 'encoder.features.7.0.block.0.1.running_var', 'encoder.features.7.0.block.0.1.num_batches_tracked', 'encoder.features.7.0.block.1.0.weight', 'encoder.features.7.0.block.1.1.weight', 'encoder.features.7.0.block.1.1.bias', 'encoder.features.7.0.block.1.1.running_mean', 'encoder.features.7.0.block.1.1.running_var', 'encoder.features.7.0.block.1.1.num_batches_tracked', 'encoder.features.7.0.block.2.fc1.weight', 'encoder.features.7.0.block.2.fc1.bias', 'encoder.features.7.0.block.2.fc2.weight', 'encoder.features.7.0.block.2.fc2.bias', 'encoder.features.7.0.block.3.0.weight', 'encoder.features.7.0.block.3.1.weight', 'encoder.features.7.0.block.3.1.bias', 'encoder.features.7.0.block.3.1.running_mean', 'encoder.features.7.0.block.3.1.running_var', 'encoder.features.7.0.block.3.1.num_batches_tracked', 'encoder.features.8.0.weight', 'encoder.features.8.1.weight', 'encoder.features.8.1.bias', 'encoder.features.8.1.running_mean', 'encoder.features.8.1.running_var', 'encoder.features.8.1.num_batches_tracked', 'projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias']\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█| 670/670 [05:45<00:00,  1.94it/s, loss=1.8441, ce=1.8440, cons=0.0011, lr=1.2e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 | loss=1.8826 | ce=1.8825 | cons=0.0006\n",
      "[Val]   Epoch 1 | loss=1.8635 | acc=21.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|█| 670/670 [05:28<00:00,  2.04it/s, loss=1.7727, ce=1.7726, cons=0.0011, lr=2.4e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 | loss=1.8644 | ce=1.8643 | cons=0.0008\n",
      "[Val]   Epoch 2 | loss=1.9276 | acc=20.55%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=1.7422, ce=1.7421, cons=0.0009, lr=3.0e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3 | loss=1.8489 | ce=1.8488 | cons=0.0012\n",
      "[Val]   Epoch 3 | loss=1.8949 | acc=28.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|█| 670/670 [06:17<00:00,  1.77it/s, loss=1.7421, ce=1.7418, cons=0.0025, lr=3.0e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4 | loss=1.7658 | ce=1.7655 | cons=0.0027\n",
      "[Val]   Epoch 4 | loss=1.6767 | acc=34.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|█| 670/670 [06:55<00:00,  1.61it/s, loss=1.5939, ce=1.5935, cons=0.0048, lr=2.9e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5 | loss=1.6771 | ce=1.6768 | cons=0.0033\n",
      "[Val]   Epoch 5 | loss=1.6209 | acc=37.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|█| 670/670 [05:31<00:00,  2.02it/s, loss=1.5622, ce=1.5617, cons=0.0058, lr=2.8e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6 | loss=1.6244 | ce=1.6240 | cons=0.0039\n",
      "[Val]   Epoch 6 | loss=1.5410 | acc=42.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|█| 670/670 [05:28<00:00,  2.04it/s, loss=1.4158, ce=1.4155, cons=0.0038, lr=2.7e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7 | loss=1.5639 | ce=1.5635 | cons=0.0045\n",
      "[Val]   Epoch 7 | loss=1.4935 | acc=45.19%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=1.1338, ce=1.1328, cons=0.0095, lr=2.6e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8 | loss=1.5171 | ce=1.5166 | cons=0.0053\n",
      "[Val]   Epoch 8 | loss=1.4520 | acc=48.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|█| 670/670 [05:24<00:00,  2.06it/s, loss=1.6102, ce=1.6095, cons=0.0065, lr=2.5e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9 | loss=1.4593 | ce=1.4586 | cons=0.0067\n",
      "[Val]   Epoch 9 | loss=1.3800 | acc=53.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|█| 670/670 [05:27<00:00,  2.05it/s, loss=1.2689, ce=1.2686, cons=0.0027, lr=2.3e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10 | loss=1.4031 | ce=1.4023 | cons=0.0079\n",
      "[Val]   Epoch 10 | loss=1.3355 | acc=55.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|█| 670/670 [05:30<00:00,  2.03it/s, loss=1.4511, ce=1.4501, cons=0.0104, lr=2.2e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11 | loss=1.3410 | ce=1.3401 | cons=0.0091\n",
      "[Val]   Epoch 11 | loss=1.3341 | acc=55.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|█| 670/670 [05:35<00:00,  2.00it/s, loss=1.1549, ce=1.1540, cons=0.0098, lr=2.0e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12 | loss=1.2810 | ce=1.2799 | cons=0.0105\n",
      "[Val]   Epoch 12 | loss=1.2976 | acc=58.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|█| 670/670 [05:32<00:00,  2.02it/s, loss=1.0355, ce=1.0346, cons=0.0089, lr=1.8e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13 | loss=1.2304 | ce=1.2293 | cons=0.0117\n",
      "[Val]   Epoch 13 | loss=1.2415 | acc=60.24%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|█| 670/670 [05:35<00:00,  2.00it/s, loss=1.6070, ce=1.6063, cons=0.0066, lr=1.6e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14 | loss=1.1831 | ce=1.1819 | cons=0.0122\n",
      "[Val]   Epoch 14 | loss=1.2347 | acc=60.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|█| 670/670 [05:35<00:00,  2.00it/s, loss=1.0023, ce=1.0014, cons=0.0090, lr=1.4e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15 | loss=1.1313 | ce=1.1299 | cons=0.0135\n",
      "[Val]   Epoch 15 | loss=1.2146 | acc=62.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|█| 670/670 [05:30<00:00,  2.03it/s, loss=0.9972, ce=0.9957, cons=0.0149, lr=1.2e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16 | loss=1.0914 | ce=1.0900 | cons=0.0138\n",
      "[Val]   Epoch 16 | loss=1.2101 | acc=62.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.2916, ce=1.2895, cons=0.0207, lr=1.1e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17 | loss=1.0532 | ce=1.0518 | cons=0.0147\n",
      "[Val]   Epoch 17 | loss=1.1908 | acc=63.63%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=1.1685, ce=1.1680, cons=0.0057, lr=9.0e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18 | loss=1.0163 | ce=1.0147 | cons=0.0156\n",
      "[Val]   Epoch 18 | loss=1.1812 | acc=66.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|█| 670/670 [05:21<00:00,  2.08it/s, loss=0.8239, ce=0.8216, cons=0.0234, lr=7.5e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19 | loss=0.9748 | ce=0.9732 | cons=0.0162\n",
      "[Val]   Epoch 19 | loss=1.1938 | acc=65.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=0.9485, ce=0.9477, cons=0.0084, lr=6.2e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20 | loss=0.9489 | ce=0.9472 | cons=0.0166\n",
      "[Val]   Epoch 20 | loss=1.2000 | acc=65.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=0.6293, ce=0.6285, cons=0.0080, lr=5.1e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21 | loss=0.9238 | ce=0.9221 | cons=0.0174\n",
      "[Val]   Epoch 21 | loss=1.1981 | acc=65.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.0718, ce=1.0691, cons=0.0271, lr=4.2e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22 | loss=0.8906 | ce=0.8888 | cons=0.0178\n",
      "[Val]   Epoch 22 | loss=1.1975 | acc=66.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|█| 670/670 [05:18<00:00,  2.10it/s, loss=0.8362, ce=0.8352, cons=0.0101, lr=3.5e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23 | loss=0.8732 | ce=0.8715 | cons=0.0177\n",
      "[Val]   Epoch 23 | loss=1.1979 | acc=66.19%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.0682, ce=1.0672, cons=0.0097, lr=3.1e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24 | loss=0.8566 | ce=0.8548 | cons=0.0177\n",
      "[Val]   Epoch 24 | loss=1.2092 | acc=66.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=0.8065, ce=0.8053, cons=0.0117, lr=3.0e-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25 | loss=0.8365 | ce=0.8347 | cons=0.0175\n",
      "[Val]   Epoch 25 | loss=1.2198 | acc=66.12%\n",
      "\n",
      "Saved: affectnet_finetuned_consistency_mac_real_time.pth\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Collate function — Permit output in PIL or Tensor format\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)  # imgs: PIL.Image list, labels: int list\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "\n",
    "# 1. Data augmentation (version tuned for real-time processing)\n",
    "# Enhanced Standard Training: Slight Rotation + Flip\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Weaker strong_aug: Simulates only the slight variations of a real camera.\n",
    "strong_aug = transforms.Compose([\n",
    "    transforms.ColorJitter(0.15, 0.15, 0.15, 0.05),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Verification enhancement: Perform only resizing and standardisation.\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 2. dataset & DataLoader\n",
    "# Train without applying the transformation, so that we can generate both weak and strong versions ourselves.\n",
    "train_set = datasets.ImageFolder(\"YOLO_format_cls/train\", transform=None)\n",
    "val_set   = datasets.ImageFolder(\"YOLO_format_cls/valid\", transform=val_aug)\n",
    "\n",
    "BATCH_SIZE = 16 \n",
    "EPOCHS     = 25\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}\")\n",
    "\n",
    "# 3. Load TinySimCLR EfficientNet-B0 weights\n",
    "base = efficientnet_b0(weights=None)\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "simclr_weights = torch.load(\"tinysimclr_effb0_mac.pth\", map_location=\"cpu\")\n",
    "missing, unexpected = base.load_state_dict(simclr_weights, strict=False)\n",
    "print(\"Loaded SimCLR backbone. Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# 4. Add category headers\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "model = nn.Sequential(\n",
    "    base,\n",
    "    nn.Linear(1280, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Label smoothing renders the output smoother and more conducive to real-time applications.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "BASE_LR = 3e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "\n",
    "# consistency loss reduces the weight slightly for greater stability.\n",
    "LAMBDA_CONS = 0.1\n",
    "\n",
    "\n",
    "# 5. Consistency loss\n",
    "def consistency_loss(logits1, logits2):\n",
    "    p1 = torch.softmax(logits1, dim=1)\n",
    "    p2 = torch.softmax(logits2, dim=1)\n",
    "    return torch.mean((p1 - p2) ** 2)\n",
    "\n",
    "# 6. lr: warmup + cosine decay\n",
    "total_steps  = EPOCHS * len(train_loader)\n",
    "warmup_ratio = 0.1          # top 10% step warmup\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup: from 0 to BASE_LR\n",
    "        return BASE_LR * float(step + 1) / float(warmup_steps + 1)\n",
    "    # Subsequently, the cosine decays to BASE_LR multiplied by 0.1.\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    min_lr = BASE_LR * 0.1\n",
    "    return min_lr + (BASE_LR - min_lr) * cosine\n",
    "\n",
    "\n",
    "# 7. Training cycle (real-time tuned)\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    ce_loss_sum = 0.0\n",
    "    cons_loss_sum = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=100)\n",
    "    for imgs, labels in pbar:\n",
    "        # imgs: PIL list, labels: Python int list\n",
    "        # weak aug (used for supervised CE loss)\n",
    "        x = torch.stack([train_aug(img) for img in imgs]).to(device)\n",
    "        y = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # strong aug (used for consistency)\n",
    "        x_strong = torch.stack([strong_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "        # Dynamically adjusting the learning rate（warmup + cosine）\n",
    "        lr = get_lr(global_step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Forward\n",
    "        logits = model(x)\n",
    "        logits_strong = model(x_strong)\n",
    "\n",
    "        ce = criterion(logits, y)\n",
    "        cons = consistency_loss(logits, logits_strong)\n",
    "\n",
    "        loss = ce + LAMBDA_CONS * cons\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Optional: Gradient cropping for further stabilisation.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        ce_loss_sum += ce.item()\n",
    "        cons_loss_sum += cons.item()\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"ce\":   f\"{ce.item():.4f}\",\n",
    "            \"cons\": f\"{cons.item():.4f}\",\n",
    "            \"lr\":   f\"{lr:.1e}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_ce   = ce_loss_sum / len(train_loader)\n",
    "    avg_cons = cons_loss_sum / len(train_loader)\n",
    "    print(f\"[Train] Epoch {epoch+1} | loss={avg_loss:.4f} | ce={avg_ce:.4f} | cons={avg_cons:.4f}\")\n",
    "\n",
    "    # Simple Verification\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            if isinstance(imgs[0], torch.Tensor):\n",
    "                x_val = torch.stack(imgs).to(device)\n",
    "            else:\n",
    "                x_val = torch.stack([val_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "            y_val = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "            logits_val = model(x_val)\n",
    "            loss_val = criterion(logits_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = logits_val.argmax(dim=1)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "            total += len(y_val)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Val]   Epoch {epoch+1} | loss={val_loss:.4f} | acc={val_acc*100:.2f}%\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), \"affectnet_finetuned_consistency_mac_real_time.pth\")\n",
    "print(\"Saved: affectnet_finetuned_consistency_mac_real_time.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec773631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train size (7-class): 10714, Val size: 3129\n",
      "5-class counts in train: Counter({3: 4170, 0: 3891, 2: 1582, 4: 744, 1: 327})\n",
      "Class weights (for CE): [0.23232516646385193, 2.764456272125244, 0.5714141726493835, 0.2167811095714569, 1.215023159980774]\n",
      "Loaded SimCLR backbone. Missing keys: ['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var', 'features.1.0.block.1.fc1.weight', 'features.1.0.block.1.fc1.bias', 'features.1.0.block.1.fc2.weight', 'features.1.0.block.1.fc2.bias', 'features.1.0.block.2.0.weight', 'features.1.0.block.2.1.weight', 'features.1.0.block.2.1.bias', 'features.1.0.block.2.1.running_mean', 'features.1.0.block.2.1.running_var', 'features.2.0.block.0.0.weight', 'features.2.0.block.0.1.weight', 'features.2.0.block.0.1.bias', 'features.2.0.block.0.1.running_mean', 'features.2.0.block.0.1.running_var', 'features.2.0.block.1.0.weight', 'features.2.0.block.1.1.weight', 'features.2.0.block.1.1.bias', 'features.2.0.block.1.1.running_mean', 'features.2.0.block.1.1.running_var', 'features.2.0.block.2.fc1.weight', 'features.2.0.block.2.fc1.bias', 'features.2.0.block.2.fc2.weight', 'features.2.0.block.2.fc2.bias', 'features.2.0.block.3.0.weight', 'features.2.0.block.3.1.weight', 'features.2.0.block.3.1.bias', 'features.2.0.block.3.1.running_mean', 'features.2.0.block.3.1.running_var', 'features.2.1.block.0.0.weight', 'features.2.1.block.0.1.weight', 'features.2.1.block.0.1.bias', 'features.2.1.block.0.1.running_mean', 'features.2.1.block.0.1.running_var', 'features.2.1.block.1.0.weight', 'features.2.1.block.1.1.weight', 'features.2.1.block.1.1.bias', 'features.2.1.block.1.1.running_mean', 'features.2.1.block.1.1.running_var', 'features.2.1.block.2.fc1.weight', 'features.2.1.block.2.fc1.bias', 'features.2.1.block.2.fc2.weight', 'features.2.1.block.2.fc2.bias', 'features.2.1.block.3.0.weight', 'features.2.1.block.3.1.weight', 'features.2.1.block.3.1.bias', 'features.2.1.block.3.1.running_mean', 'features.2.1.block.3.1.running_var', 'features.3.0.block.0.0.weight', 'features.3.0.block.0.1.weight', 'features.3.0.block.0.1.bias', 'features.3.0.block.0.1.running_mean', 'features.3.0.block.0.1.running_var', 'features.3.0.block.1.0.weight', 'features.3.0.block.1.1.weight', 'features.3.0.block.1.1.bias', 'features.3.0.block.1.1.running_mean', 'features.3.0.block.1.1.running_var', 'features.3.0.block.2.fc1.weight', 'features.3.0.block.2.fc1.bias', 'features.3.0.block.2.fc2.weight', 'features.3.0.block.2.fc2.bias', 'features.3.0.block.3.0.weight', 'features.3.0.block.3.1.weight', 'features.3.0.block.3.1.bias', 'features.3.0.block.3.1.running_mean', 'features.3.0.block.3.1.running_var', 'features.3.1.block.0.0.weight', 'features.3.1.block.0.1.weight', 'features.3.1.block.0.1.bias', 'features.3.1.block.0.1.running_mean', 'features.3.1.block.0.1.running_var', 'features.3.1.block.1.0.weight', 'features.3.1.block.1.1.weight', 'features.3.1.block.1.1.bias', 'features.3.1.block.1.1.running_mean', 'features.3.1.block.1.1.running_var', 'features.3.1.block.2.fc1.weight', 'features.3.1.block.2.fc1.bias', 'features.3.1.block.2.fc2.weight', 'features.3.1.block.2.fc2.bias', 'features.3.1.block.3.0.weight', 'features.3.1.block.3.1.weight', 'features.3.1.block.3.1.bias', 'features.3.1.block.3.1.running_mean', 'features.3.1.block.3.1.running_var', 'features.4.0.block.0.0.weight', 'features.4.0.block.0.1.weight', 'features.4.0.block.0.1.bias', 'features.4.0.block.0.1.running_mean', 'features.4.0.block.0.1.running_var', 'features.4.0.block.1.0.weight', 'features.4.0.block.1.1.weight', 'features.4.0.block.1.1.bias', 'features.4.0.block.1.1.running_mean', 'features.4.0.block.1.1.running_var', 'features.4.0.block.2.fc1.weight', 'features.4.0.block.2.fc1.bias', 'features.4.0.block.2.fc2.weight', 'features.4.0.block.2.fc2.bias', 'features.4.0.block.3.0.weight', 'features.4.0.block.3.1.weight', 'features.4.0.block.3.1.bias', 'features.4.0.block.3.1.running_mean', 'features.4.0.block.3.1.running_var', 'features.4.1.block.0.0.weight', 'features.4.1.block.0.1.weight', 'features.4.1.block.0.1.bias', 'features.4.1.block.0.1.running_mean', 'features.4.1.block.0.1.running_var', 'features.4.1.block.1.0.weight', 'features.4.1.block.1.1.weight', 'features.4.1.block.1.1.bias', 'features.4.1.block.1.1.running_mean', 'features.4.1.block.1.1.running_var', 'features.4.1.block.2.fc1.weight', 'features.4.1.block.2.fc1.bias', 'features.4.1.block.2.fc2.weight', 'features.4.1.block.2.fc2.bias', 'features.4.1.block.3.0.weight', 'features.4.1.block.3.1.weight', 'features.4.1.block.3.1.bias', 'features.4.1.block.3.1.running_mean', 'features.4.1.block.3.1.running_var', 'features.4.2.block.0.0.weight', 'features.4.2.block.0.1.weight', 'features.4.2.block.0.1.bias', 'features.4.2.block.0.1.running_mean', 'features.4.2.block.0.1.running_var', 'features.4.2.block.1.0.weight', 'features.4.2.block.1.1.weight', 'features.4.2.block.1.1.bias', 'features.4.2.block.1.1.running_mean', 'features.4.2.block.1.1.running_var', 'features.4.2.block.2.fc1.weight', 'features.4.2.block.2.fc1.bias', 'features.4.2.block.2.fc2.weight', 'features.4.2.block.2.fc2.bias', 'features.4.2.block.3.0.weight', 'features.4.2.block.3.1.weight', 'features.4.2.block.3.1.bias', 'features.4.2.block.3.1.running_mean', 'features.4.2.block.3.1.running_var', 'features.5.0.block.0.0.weight', 'features.5.0.block.0.1.weight', 'features.5.0.block.0.1.bias', 'features.5.0.block.0.1.running_mean', 'features.5.0.block.0.1.running_var', 'features.5.0.block.1.0.weight', 'features.5.0.block.1.1.weight', 'features.5.0.block.1.1.bias', 'features.5.0.block.1.1.running_mean', 'features.5.0.block.1.1.running_var', 'features.5.0.block.2.fc1.weight', 'features.5.0.block.2.fc1.bias', 'features.5.0.block.2.fc2.weight', 'features.5.0.block.2.fc2.bias', 'features.5.0.block.3.0.weight', 'features.5.0.block.3.1.weight', 'features.5.0.block.3.1.bias', 'features.5.0.block.3.1.running_mean', 'features.5.0.block.3.1.running_var', 'features.5.1.block.0.0.weight', 'features.5.1.block.0.1.weight', 'features.5.1.block.0.1.bias', 'features.5.1.block.0.1.running_mean', 'features.5.1.block.0.1.running_var', 'features.5.1.block.1.0.weight', 'features.5.1.block.1.1.weight', 'features.5.1.block.1.1.bias', 'features.5.1.block.1.1.running_mean', 'features.5.1.block.1.1.running_var', 'features.5.1.block.2.fc1.weight', 'features.5.1.block.2.fc1.bias', 'features.5.1.block.2.fc2.weight', 'features.5.1.block.2.fc2.bias', 'features.5.1.block.3.0.weight', 'features.5.1.block.3.1.weight', 'features.5.1.block.3.1.bias', 'features.5.1.block.3.1.running_mean', 'features.5.1.block.3.1.running_var', 'features.5.2.block.0.0.weight', 'features.5.2.block.0.1.weight', 'features.5.2.block.0.1.bias', 'features.5.2.block.0.1.running_mean', 'features.5.2.block.0.1.running_var', 'features.5.2.block.1.0.weight', 'features.5.2.block.1.1.weight', 'features.5.2.block.1.1.bias', 'features.5.2.block.1.1.running_mean', 'features.5.2.block.1.1.running_var', 'features.5.2.block.2.fc1.weight', 'features.5.2.block.2.fc1.bias', 'features.5.2.block.2.fc2.weight', 'features.5.2.block.2.fc2.bias', 'features.5.2.block.3.0.weight', 'features.5.2.block.3.1.weight', 'features.5.2.block.3.1.bias', 'features.5.2.block.3.1.running_mean', 'features.5.2.block.3.1.running_var', 'features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.0.1.running_mean', 'features.6.0.block.0.1.running_var', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.1.1.running_mean', 'features.6.0.block.1.1.running_var', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.0.block.3.1.running_mean', 'features.6.0.block.3.1.running_var', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.0.1.running_mean', 'features.6.1.block.0.1.running_var', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.1.1.running_mean', 'features.6.1.block.1.1.running_var', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.1.block.3.1.running_mean', 'features.6.1.block.3.1.running_var', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.0.1.running_mean', 'features.6.2.block.0.1.running_var', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.1.1.running_mean', 'features.6.2.block.1.1.running_var', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.2.block.3.1.running_mean', 'features.6.2.block.3.1.running_var', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.0.1.running_mean', 'features.6.3.block.0.1.running_var', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.1.1.running_mean', 'features.6.3.block.1.1.running_var', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.3.block.3.1.running_mean', 'features.6.3.block.3.1.running_var', 'features.7.0.block.0.0.weight', 'features.7.0.block.0.1.weight', 'features.7.0.block.0.1.bias', 'features.7.0.block.0.1.running_mean', 'features.7.0.block.0.1.running_var', 'features.7.0.block.1.0.weight', 'features.7.0.block.1.1.weight', 'features.7.0.block.1.1.bias', 'features.7.0.block.1.1.running_mean', 'features.7.0.block.1.1.running_var', 'features.7.0.block.2.fc1.weight', 'features.7.0.block.2.fc1.bias', 'features.7.0.block.2.fc2.weight', 'features.7.0.block.2.fc2.bias', 'features.7.0.block.3.0.weight', 'features.7.0.block.3.1.weight', 'features.7.0.block.3.1.bias', 'features.7.0.block.3.1.running_mean', 'features.7.0.block.3.1.running_var', 'features.8.0.weight', 'features.8.1.weight', 'features.8.1.bias', 'features.8.1.running_mean', 'features.8.1.running_var']\n",
      "Unexpected keys: ['encoder.features.0.0.weight', 'encoder.features.0.1.weight', 'encoder.features.0.1.bias', 'encoder.features.0.1.running_mean', 'encoder.features.0.1.running_var', 'encoder.features.0.1.num_batches_tracked', 'encoder.features.1.0.block.0.0.weight', 'encoder.features.1.0.block.0.1.weight', 'encoder.features.1.0.block.0.1.bias', 'encoder.features.1.0.block.0.1.running_mean', 'encoder.features.1.0.block.0.1.running_var', 'encoder.features.1.0.block.0.1.num_batches_tracked', 'encoder.features.1.0.block.1.fc1.weight', 'encoder.features.1.0.block.1.fc1.bias', 'encoder.features.1.0.block.1.fc2.weight', 'encoder.features.1.0.block.1.fc2.bias', 'encoder.features.1.0.block.2.0.weight', 'encoder.features.1.0.block.2.1.weight', 'encoder.features.1.0.block.2.1.bias', 'encoder.features.1.0.block.2.1.running_mean', 'encoder.features.1.0.block.2.1.running_var', 'encoder.features.1.0.block.2.1.num_batches_tracked', 'encoder.features.2.0.block.0.0.weight', 'encoder.features.2.0.block.0.1.weight', 'encoder.features.2.0.block.0.1.bias', 'encoder.features.2.0.block.0.1.running_mean', 'encoder.features.2.0.block.0.1.running_var', 'encoder.features.2.0.block.0.1.num_batches_tracked', 'encoder.features.2.0.block.1.0.weight', 'encoder.features.2.0.block.1.1.weight', 'encoder.features.2.0.block.1.1.bias', 'encoder.features.2.0.block.1.1.running_mean', 'encoder.features.2.0.block.1.1.running_var', 'encoder.features.2.0.block.1.1.num_batches_tracked', 'encoder.features.2.0.block.2.fc1.weight', 'encoder.features.2.0.block.2.fc1.bias', 'encoder.features.2.0.block.2.fc2.weight', 'encoder.features.2.0.block.2.fc2.bias', 'encoder.features.2.0.block.3.0.weight', 'encoder.features.2.0.block.3.1.weight', 'encoder.features.2.0.block.3.1.bias', 'encoder.features.2.0.block.3.1.running_mean', 'encoder.features.2.0.block.3.1.running_var', 'encoder.features.2.0.block.3.1.num_batches_tracked', 'encoder.features.2.1.block.0.0.weight', 'encoder.features.2.1.block.0.1.weight', 'encoder.features.2.1.block.0.1.bias', 'encoder.features.2.1.block.0.1.running_mean', 'encoder.features.2.1.block.0.1.running_var', 'encoder.features.2.1.block.0.1.num_batches_tracked', 'encoder.features.2.1.block.1.0.weight', 'encoder.features.2.1.block.1.1.weight', 'encoder.features.2.1.block.1.1.bias', 'encoder.features.2.1.block.1.1.running_mean', 'encoder.features.2.1.block.1.1.running_var', 'encoder.features.2.1.block.1.1.num_batches_tracked', 'encoder.features.2.1.block.2.fc1.weight', 'encoder.features.2.1.block.2.fc1.bias', 'encoder.features.2.1.block.2.fc2.weight', 'encoder.features.2.1.block.2.fc2.bias', 'encoder.features.2.1.block.3.0.weight', 'encoder.features.2.1.block.3.1.weight', 'encoder.features.2.1.block.3.1.bias', 'encoder.features.2.1.block.3.1.running_mean', 'encoder.features.2.1.block.3.1.running_var', 'encoder.features.2.1.block.3.1.num_batches_tracked', 'encoder.features.3.0.block.0.0.weight', 'encoder.features.3.0.block.0.1.weight', 'encoder.features.3.0.block.0.1.bias', 'encoder.features.3.0.block.0.1.running_mean', 'encoder.features.3.0.block.0.1.running_var', 'encoder.features.3.0.block.0.1.num_batches_tracked', 'encoder.features.3.0.block.1.0.weight', 'encoder.features.3.0.block.1.1.weight', 'encoder.features.3.0.block.1.1.bias', 'encoder.features.3.0.block.1.1.running_mean', 'encoder.features.3.0.block.1.1.running_var', 'encoder.features.3.0.block.1.1.num_batches_tracked', 'encoder.features.3.0.block.2.fc1.weight', 'encoder.features.3.0.block.2.fc1.bias', 'encoder.features.3.0.block.2.fc2.weight', 'encoder.features.3.0.block.2.fc2.bias', 'encoder.features.3.0.block.3.0.weight', 'encoder.features.3.0.block.3.1.weight', 'encoder.features.3.0.block.3.1.bias', 'encoder.features.3.0.block.3.1.running_mean', 'encoder.features.3.0.block.3.1.running_var', 'encoder.features.3.0.block.3.1.num_batches_tracked', 'encoder.features.3.1.block.0.0.weight', 'encoder.features.3.1.block.0.1.weight', 'encoder.features.3.1.block.0.1.bias', 'encoder.features.3.1.block.0.1.running_mean', 'encoder.features.3.1.block.0.1.running_var', 'encoder.features.3.1.block.0.1.num_batches_tracked', 'encoder.features.3.1.block.1.0.weight', 'encoder.features.3.1.block.1.1.weight', 'encoder.features.3.1.block.1.1.bias', 'encoder.features.3.1.block.1.1.running_mean', 'encoder.features.3.1.block.1.1.running_var', 'encoder.features.3.1.block.1.1.num_batches_tracked', 'encoder.features.3.1.block.2.fc1.weight', 'encoder.features.3.1.block.2.fc1.bias', 'encoder.features.3.1.block.2.fc2.weight', 'encoder.features.3.1.block.2.fc2.bias', 'encoder.features.3.1.block.3.0.weight', 'encoder.features.3.1.block.3.1.weight', 'encoder.features.3.1.block.3.1.bias', 'encoder.features.3.1.block.3.1.running_mean', 'encoder.features.3.1.block.3.1.running_var', 'encoder.features.3.1.block.3.1.num_batches_tracked', 'encoder.features.4.0.block.0.0.weight', 'encoder.features.4.0.block.0.1.weight', 'encoder.features.4.0.block.0.1.bias', 'encoder.features.4.0.block.0.1.running_mean', 'encoder.features.4.0.block.0.1.running_var', 'encoder.features.4.0.block.0.1.num_batches_tracked', 'encoder.features.4.0.block.1.0.weight', 'encoder.features.4.0.block.1.1.weight', 'encoder.features.4.0.block.1.1.bias', 'encoder.features.4.0.block.1.1.running_mean', 'encoder.features.4.0.block.1.1.running_var', 'encoder.features.4.0.block.1.1.num_batches_tracked', 'encoder.features.4.0.block.2.fc1.weight', 'encoder.features.4.0.block.2.fc1.bias', 'encoder.features.4.0.block.2.fc2.weight', 'encoder.features.4.0.block.2.fc2.bias', 'encoder.features.4.0.block.3.0.weight', 'encoder.features.4.0.block.3.1.weight', 'encoder.features.4.0.block.3.1.bias', 'encoder.features.4.0.block.3.1.running_mean', 'encoder.features.4.0.block.3.1.running_var', 'encoder.features.4.0.block.3.1.num_batches_tracked', 'encoder.features.4.1.block.0.0.weight', 'encoder.features.4.1.block.0.1.weight', 'encoder.features.4.1.block.0.1.bias', 'encoder.features.4.1.block.0.1.running_mean', 'encoder.features.4.1.block.0.1.running_var', 'encoder.features.4.1.block.0.1.num_batches_tracked', 'encoder.features.4.1.block.1.0.weight', 'encoder.features.4.1.block.1.1.weight', 'encoder.features.4.1.block.1.1.bias', 'encoder.features.4.1.block.1.1.running_mean', 'encoder.features.4.1.block.1.1.running_var', 'encoder.features.4.1.block.1.1.num_batches_tracked', 'encoder.features.4.1.block.2.fc1.weight', 'encoder.features.4.1.block.2.fc1.bias', 'encoder.features.4.1.block.2.fc2.weight', 'encoder.features.4.1.block.2.fc2.bias', 'encoder.features.4.1.block.3.0.weight', 'encoder.features.4.1.block.3.1.weight', 'encoder.features.4.1.block.3.1.bias', 'encoder.features.4.1.block.3.1.running_mean', 'encoder.features.4.1.block.3.1.running_var', 'encoder.features.4.1.block.3.1.num_batches_tracked', 'encoder.features.4.2.block.0.0.weight', 'encoder.features.4.2.block.0.1.weight', 'encoder.features.4.2.block.0.1.bias', 'encoder.features.4.2.block.0.1.running_mean', 'encoder.features.4.2.block.0.1.running_var', 'encoder.features.4.2.block.0.1.num_batches_tracked', 'encoder.features.4.2.block.1.0.weight', 'encoder.features.4.2.block.1.1.weight', 'encoder.features.4.2.block.1.1.bias', 'encoder.features.4.2.block.1.1.running_mean', 'encoder.features.4.2.block.1.1.running_var', 'encoder.features.4.2.block.1.1.num_batches_tracked', 'encoder.features.4.2.block.2.fc1.weight', 'encoder.features.4.2.block.2.fc1.bias', 'encoder.features.4.2.block.2.fc2.weight', 'encoder.features.4.2.block.2.fc2.bias', 'encoder.features.4.2.block.3.0.weight', 'encoder.features.4.2.block.3.1.weight', 'encoder.features.4.2.block.3.1.bias', 'encoder.features.4.2.block.3.1.running_mean', 'encoder.features.4.2.block.3.1.running_var', 'encoder.features.4.2.block.3.1.num_batches_tracked', 'encoder.features.5.0.block.0.0.weight', 'encoder.features.5.0.block.0.1.weight', 'encoder.features.5.0.block.0.1.bias', 'encoder.features.5.0.block.0.1.running_mean', 'encoder.features.5.0.block.0.1.running_var', 'encoder.features.5.0.block.0.1.num_batches_tracked', 'encoder.features.5.0.block.1.0.weight', 'encoder.features.5.0.block.1.1.weight', 'encoder.features.5.0.block.1.1.bias', 'encoder.features.5.0.block.1.1.running_mean', 'encoder.features.5.0.block.1.1.running_var', 'encoder.features.5.0.block.1.1.num_batches_tracked', 'encoder.features.5.0.block.2.fc1.weight', 'encoder.features.5.0.block.2.fc1.bias', 'encoder.features.5.0.block.2.fc2.weight', 'encoder.features.5.0.block.2.fc2.bias', 'encoder.features.5.0.block.3.0.weight', 'encoder.features.5.0.block.3.1.weight', 'encoder.features.5.0.block.3.1.bias', 'encoder.features.5.0.block.3.1.running_mean', 'encoder.features.5.0.block.3.1.running_var', 'encoder.features.5.0.block.3.1.num_batches_tracked', 'encoder.features.5.1.block.0.0.weight', 'encoder.features.5.1.block.0.1.weight', 'encoder.features.5.1.block.0.1.bias', 'encoder.features.5.1.block.0.1.running_mean', 'encoder.features.5.1.block.0.1.running_var', 'encoder.features.5.1.block.0.1.num_batches_tracked', 'encoder.features.5.1.block.1.0.weight', 'encoder.features.5.1.block.1.1.weight', 'encoder.features.5.1.block.1.1.bias', 'encoder.features.5.1.block.1.1.running_mean', 'encoder.features.5.1.block.1.1.running_var', 'encoder.features.5.1.block.1.1.num_batches_tracked', 'encoder.features.5.1.block.2.fc1.weight', 'encoder.features.5.1.block.2.fc1.bias', 'encoder.features.5.1.block.2.fc2.weight', 'encoder.features.5.1.block.2.fc2.bias', 'encoder.features.5.1.block.3.0.weight', 'encoder.features.5.1.block.3.1.weight', 'encoder.features.5.1.block.3.1.bias', 'encoder.features.5.1.block.3.1.running_mean', 'encoder.features.5.1.block.3.1.running_var', 'encoder.features.5.1.block.3.1.num_batches_tracked', 'encoder.features.5.2.block.0.0.weight', 'encoder.features.5.2.block.0.1.weight', 'encoder.features.5.2.block.0.1.bias', 'encoder.features.5.2.block.0.1.running_mean', 'encoder.features.5.2.block.0.1.running_var', 'encoder.features.5.2.block.0.1.num_batches_tracked', 'encoder.features.5.2.block.1.0.weight', 'encoder.features.5.2.block.1.1.weight', 'encoder.features.5.2.block.1.1.bias', 'encoder.features.5.2.block.1.1.running_mean', 'encoder.features.5.2.block.1.1.running_var', 'encoder.features.5.2.block.1.1.num_batches_tracked', 'encoder.features.5.2.block.2.fc1.weight', 'encoder.features.5.2.block.2.fc1.bias', 'encoder.features.5.2.block.2.fc2.weight', 'encoder.features.5.2.block.2.fc2.bias', 'encoder.features.5.2.block.3.0.weight', 'encoder.features.5.2.block.3.1.weight', 'encoder.features.5.2.block.3.1.bias', 'encoder.features.5.2.block.3.1.running_mean', 'encoder.features.5.2.block.3.1.running_var', 'encoder.features.5.2.block.3.1.num_batches_tracked', 'encoder.features.6.0.block.0.0.weight', 'encoder.features.6.0.block.0.1.weight', 'encoder.features.6.0.block.0.1.bias', 'encoder.features.6.0.block.0.1.running_mean', 'encoder.features.6.0.block.0.1.running_var', 'encoder.features.6.0.block.0.1.num_batches_tracked', 'encoder.features.6.0.block.1.0.weight', 'encoder.features.6.0.block.1.1.weight', 'encoder.features.6.0.block.1.1.bias', 'encoder.features.6.0.block.1.1.running_mean', 'encoder.features.6.0.block.1.1.running_var', 'encoder.features.6.0.block.1.1.num_batches_tracked', 'encoder.features.6.0.block.2.fc1.weight', 'encoder.features.6.0.block.2.fc1.bias', 'encoder.features.6.0.block.2.fc2.weight', 'encoder.features.6.0.block.2.fc2.bias', 'encoder.features.6.0.block.3.0.weight', 'encoder.features.6.0.block.3.1.weight', 'encoder.features.6.0.block.3.1.bias', 'encoder.features.6.0.block.3.1.running_mean', 'encoder.features.6.0.block.3.1.running_var', 'encoder.features.6.0.block.3.1.num_batches_tracked', 'encoder.features.6.1.block.0.0.weight', 'encoder.features.6.1.block.0.1.weight', 'encoder.features.6.1.block.0.1.bias', 'encoder.features.6.1.block.0.1.running_mean', 'encoder.features.6.1.block.0.1.running_var', 'encoder.features.6.1.block.0.1.num_batches_tracked', 'encoder.features.6.1.block.1.0.weight', 'encoder.features.6.1.block.1.1.weight', 'encoder.features.6.1.block.1.1.bias', 'encoder.features.6.1.block.1.1.running_mean', 'encoder.features.6.1.block.1.1.running_var', 'encoder.features.6.1.block.1.1.num_batches_tracked', 'encoder.features.6.1.block.2.fc1.weight', 'encoder.features.6.1.block.2.fc1.bias', 'encoder.features.6.1.block.2.fc2.weight', 'encoder.features.6.1.block.2.fc2.bias', 'encoder.features.6.1.block.3.0.weight', 'encoder.features.6.1.block.3.1.weight', 'encoder.features.6.1.block.3.1.bias', 'encoder.features.6.1.block.3.1.running_mean', 'encoder.features.6.1.block.3.1.running_var', 'encoder.features.6.1.block.3.1.num_batches_tracked', 'encoder.features.6.2.block.0.0.weight', 'encoder.features.6.2.block.0.1.weight', 'encoder.features.6.2.block.0.1.bias', 'encoder.features.6.2.block.0.1.running_mean', 'encoder.features.6.2.block.0.1.running_var', 'encoder.features.6.2.block.0.1.num_batches_tracked', 'encoder.features.6.2.block.1.0.weight', 'encoder.features.6.2.block.1.1.weight', 'encoder.features.6.2.block.1.1.bias', 'encoder.features.6.2.block.1.1.running_mean', 'encoder.features.6.2.block.1.1.running_var', 'encoder.features.6.2.block.1.1.num_batches_tracked', 'encoder.features.6.2.block.2.fc1.weight', 'encoder.features.6.2.block.2.fc1.bias', 'encoder.features.6.2.block.2.fc2.weight', 'encoder.features.6.2.block.2.fc2.bias', 'encoder.features.6.2.block.3.0.weight', 'encoder.features.6.2.block.3.1.weight', 'encoder.features.6.2.block.3.1.bias', 'encoder.features.6.2.block.3.1.running_mean', 'encoder.features.6.2.block.3.1.running_var', 'encoder.features.6.2.block.3.1.num_batches_tracked', 'encoder.features.6.3.block.0.0.weight', 'encoder.features.6.3.block.0.1.weight', 'encoder.features.6.3.block.0.1.bias', 'encoder.features.6.3.block.0.1.running_mean', 'encoder.features.6.3.block.0.1.running_var', 'encoder.features.6.3.block.0.1.num_batches_tracked', 'encoder.features.6.3.block.1.0.weight', 'encoder.features.6.3.block.1.1.weight', 'encoder.features.6.3.block.1.1.bias', 'encoder.features.6.3.block.1.1.running_mean', 'encoder.features.6.3.block.1.1.running_var', 'encoder.features.6.3.block.1.1.num_batches_tracked', 'encoder.features.6.3.block.2.fc1.weight', 'encoder.features.6.3.block.2.fc1.bias', 'encoder.features.6.3.block.2.fc2.weight', 'encoder.features.6.3.block.2.fc2.bias', 'encoder.features.6.3.block.3.0.weight', 'encoder.features.6.3.block.3.1.weight', 'encoder.features.6.3.block.3.1.bias', 'encoder.features.6.3.block.3.1.running_mean', 'encoder.features.6.3.block.3.1.running_var', 'encoder.features.6.3.block.3.1.num_batches_tracked', 'encoder.features.7.0.block.0.0.weight', 'encoder.features.7.0.block.0.1.weight', 'encoder.features.7.0.block.0.1.bias', 'encoder.features.7.0.block.0.1.running_mean', 'encoder.features.7.0.block.0.1.running_var', 'encoder.features.7.0.block.0.1.num_batches_tracked', 'encoder.features.7.0.block.1.0.weight', 'encoder.features.7.0.block.1.1.weight', 'encoder.features.7.0.block.1.1.bias', 'encoder.features.7.0.block.1.1.running_mean', 'encoder.features.7.0.block.1.1.running_var', 'encoder.features.7.0.block.1.1.num_batches_tracked', 'encoder.features.7.0.block.2.fc1.weight', 'encoder.features.7.0.block.2.fc1.bias', 'encoder.features.7.0.block.2.fc2.weight', 'encoder.features.7.0.block.2.fc2.bias', 'encoder.features.7.0.block.3.0.weight', 'encoder.features.7.0.block.3.1.weight', 'encoder.features.7.0.block.3.1.bias', 'encoder.features.7.0.block.3.1.running_mean', 'encoder.features.7.0.block.3.1.running_var', 'encoder.features.7.0.block.3.1.num_batches_tracked', 'encoder.features.8.0.weight', 'encoder.features.8.1.weight', 'encoder.features.8.1.bias', 'encoder.features.8.1.running_mean', 'encoder.features.8.1.running_var', 'encoder.features.8.1.num_batches_tracked', 'projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█| 670/670 [05:38<00:00,  1.98it/s, loss=1.2741, main=0.8555, mouth=0.8373, lr=1.2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 | loss=1.9957 | main=1.3257 | mouth=1.3401\n",
      "[Val]   Epoch 1 | loss=3.1075 | acc=3.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=2.2458, main=1.4744, mouth=1.5428, lr=2.4e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 | loss=1.8396 | main=1.2229 | mouth=1.2335\n",
      "[Val]   Epoch 2 | loss=2.7359 | acc=5.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=1.4137, main=0.8732, mouth=1.0810, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3 | loss=1.5659 | main=1.0332 | mouth=1.0653\n",
      "[Val]   Epoch 3 | loss=3.2052 | acc=8.25%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=1.6353, main=1.0019, mouth=1.2668, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4 | loss=1.3591 | main=0.8874 | mouth=0.9434\n",
      "[Val]   Epoch 4 | loss=2.9958 | acc=12.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=1.0680, main=0.6804, mouth=0.7752, lr=2.9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5 | loss=1.2728 | main=0.8224 | mouth=0.9007\n",
      "[Val]   Epoch 5 | loss=2.6494 | acc=12.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|█| 670/670 [05:26<00:00,  2.06it/s, loss=0.9665, main=0.6410, mouth=0.6510, lr=2.8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6 | loss=1.1742 | main=0.7506 | mouth=0.8471\n",
      "[Val]   Epoch 6 | loss=2.5499 | acc=19.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|█| 670/670 [05:19<00:00,  2.09it/s, loss=0.8011, main=0.5118, mouth=0.5787, lr=2.7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7 | loss=1.1077 | main=0.7056 | mouth=0.8042\n",
      "[Val]   Epoch 7 | loss=2.7075 | acc=25.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.7768, main=1.1417, mouth=1.2701, lr=2.6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8 | loss=1.0556 | main=0.6644 | mouth=0.7824\n",
      "[Val]   Epoch 8 | loss=2.5010 | acc=33.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=0.6689, main=0.4222, mouth=0.4934, lr=2.5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9 | loss=1.0401 | main=0.6521 | mouth=0.7760\n",
      "[Val]   Epoch 9 | loss=2.4204 | acc=38.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=0.6260, main=0.4373, mouth=0.3775, lr=2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10 | loss=0.9948 | main=0.6195 | mouth=0.7506\n",
      "[Val]   Epoch 10 | loss=2.1748 | acc=46.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=1.1281, main=0.5776, mouth=1.1010, lr=2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11 | loss=0.9364 | main=0.5772 | mouth=0.7185\n",
      "[Val]   Epoch 11 | loss=2.3479 | acc=40.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|█| 670/670 [05:30<00:00,  2.03it/s, loss=1.7144, main=1.0189, mouth=1.3911, lr=2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12 | loss=0.9350 | main=0.5739 | mouth=0.7220\n",
      "[Val]   Epoch 12 | loss=2.0433 | acc=51.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|█| 670/670 [05:31<00:00,  2.02it/s, loss=0.6658, main=0.3551, mouth=0.6213, lr=1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13 | loss=0.8955 | main=0.5501 | mouth=0.6907\n",
      "[Val]   Epoch 13 | loss=2.1880 | acc=47.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=0.7295, main=0.4092, mouth=0.6406, lr=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14 | loss=0.8521 | main=0.5224 | mouth=0.6594\n",
      "[Val]   Epoch 14 | loss=2.0875 | acc=51.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|█| 670/670 [05:24<00:00,  2.06it/s, loss=0.8950, main=0.5725, mouth=0.6449, lr=1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15 | loss=0.8428 | main=0.5164 | mouth=0.6528\n",
      "[Val]   Epoch 15 | loss=2.0318 | acc=56.92%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|█| 670/670 [05:27<00:00,  2.04it/s, loss=0.5221, main=0.3292, mouth=0.3858, lr=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16 | loss=0.8022 | main=0.4890 | mouth=0.6263\n",
      "[Val]   Epoch 16 | loss=1.8374 | acc=61.43%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|█| 670/670 [05:34<00:00,  2.00it/s, loss=0.6582, main=0.3894, mouth=0.5375, lr=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17 | loss=0.7837 | main=0.4735 | mouth=0.6205\n",
      "[Val]   Epoch 17 | loss=1.9848 | acc=57.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=0.6174, main=0.4186, mouth=0.3976, lr=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18 | loss=0.7667 | main=0.4669 | mouth=0.5995\n",
      "[Val]   Epoch 18 | loss=1.9119 | acc=60.95%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|█| 670/670 [05:24<00:00,  2.07it/s, loss=1.0313, main=0.6433, mouth=0.7760, lr=7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19 | loss=0.7194 | main=0.4365 | mouth=0.5659\n",
      "[Val]   Epoch 19 | loss=1.8559 | acc=63.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=1.0193, main=0.6017, mouth=0.8352, lr=6.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20 | loss=0.7272 | main=0.4442 | mouth=0.5661\n",
      "[Val]   Epoch 20 | loss=1.8626 | acc=62.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=0.5901, main=0.3841, mouth=0.4121, lr=5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21 | loss=0.7072 | main=0.4310 | mouth=0.5523\n",
      "[Val]   Epoch 21 | loss=1.8155 | acc=65.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|█| 670/670 [05:24<00:00,  2.07it/s, loss=0.4583, main=0.2941, mouth=0.3284, lr=4.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22 | loss=0.7119 | main=0.4349 | mouth=0.5540\n",
      "[Val]   Epoch 22 | loss=1.8592 | acc=63.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|█| 670/670 [05:24<00:00,  2.06it/s, loss=0.4451, main=0.2781, mouth=0.3340, lr=3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23 | loss=0.7177 | main=0.4359 | mouth=0.5635\n",
      "[Val]   Epoch 23 | loss=1.7837 | acc=66.63%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=0.6317, main=0.3921, mouth=0.4793, lr=3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24 | loss=0.6926 | main=0.4242 | mouth=0.5369\n",
      "[Val]   Epoch 24 | loss=1.8484 | acc=65.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|█| 670/670 [05:24<00:00,  2.06it/s, loss=0.7146, main=0.4796, mouth=0.4700, lr=3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25 | loss=0.6869 | main=0.4188 | mouth=0.5363\n",
      "[Val]   Epoch 25 | loss=1.7563 | acc=68.20%\n",
      "\n",
      "Saved model to affectnet_fer5_mouth.pth\n"
     ]
    }
   ],
   "source": [
    "# fine tuning v2(class weight + mouth-based auxiliary head + 5 classes)\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT   = \"YOLO_format_cls\"\n",
    "TRAIN_DIR   = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR     = os.path.join(DATA_ROOT, \"valid\")\n",
    "SIMCLR_PATH = \"tinysimclr_effb0_mac.pth\"\n",
    "SAVE_PATH   = \"affectnet_fer5_mouth.pth\"\n",
    "\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 25\n",
    "BASE_LR     = 3e-4\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# 1. collate_fn: Preserve PIL images\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)    # imgs: list[PIL.Image], labels: list[int]\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "\n",
    "# 2. Data Augmentation (Lightweight Version for Real-Time Scenarios)\n",
    "# Primary Branch: Micro-Rotation + Mirroring\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Mouth branch: Apply a similar enhancement to the mouth crop.\n",
    "mouth_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Verification: Resize only + normalisation\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 7 to 5 category mapping: original index to new 5 categories\n",
    "# 0: angry     -> 0 (Angry)\n",
    "# 1: disgusted -> 0 (Angry)\n",
    "# 2: fearful   -> 3 (Surprise)\n",
    "# 3: happy     -> 1 (Happy)\n",
    "# 4: neutral   -> 4 (Neutral)\n",
    "# 5: sad       -> 2 (Sad)\n",
    "# 6: surprised -> 3 (Surprise)\n",
    "mapping_5 = torch.tensor([0, 0, 3, 1, 4, 2, 3], dtype=torch.long)\n",
    "\n",
    "\n",
    "# 3. Mouth Crop function (based on lower half of face ROI)\n",
    "def crop_mouth_pil(img):\n",
    "    \"\"\"\n",
    "    Simple Mouth ROI: Select the lower 45% of the face\n",
    "    img: PIL.Image\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    top = int(h * 0.55)\n",
    "    # (left, upper, right, lower)\n",
    "    return img.crop((0, top, w, h))\n",
    "\n",
    "\n",
    "# 4. dataset + Balanced Sampler + class weight\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=None)\n",
    "val_set   = datasets.ImageFolder(VAL_DIR, transform=None)\n",
    "\n",
    "print(f\"Train size (7-class): {len(train_set)}, Val size: {len(val_set)}\")\n",
    "\n",
    "# First, calculate the distribution of labels after the 7 to 5 mapping for use in the sampler/class weighting.\n",
    "new_targets = []\n",
    "for _, orig_label in train_set.samples:\n",
    "    new_label = int(mapping_5[orig_label])\n",
    "    new_targets.append(new_label)\n",
    "\n",
    "counter_5 = Counter(new_targets)\n",
    "print(\"5-class counts in train:\", counter_5)\n",
    "\n",
    "# class weight: 1 / count（(Renormalisation)\n",
    "class_weights = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    class_weights.append(1.0 / counter_5[c])\n",
    "\n",
    "# Normalise (optional)\n",
    "sum_w = sum(class_weights)\n",
    "class_weights = [w / sum_w * NUM_CLASSES for w in class_weights]\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights (for CE):\", class_weights_tensor.tolist())\n",
    "\n",
    "# WeightedRandomSampler\n",
    "sample_weights = [class_weights[label] for label in new_targets]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Backbone + Mouth auxiliary head model\n",
    "class FER5WithMouth(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone  # EfficientNet-B0, classifier=Identity (output 1280)\n",
    "\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        feat_full = self.backbone(x_full)   # [B, 1280]\n",
    "        feat_mouth = self.backbone(x_mouth) # [B, 1280]\n",
    "\n",
    "        logits_main = self.head_main(feat_full)\n",
    "        logits_mouth = self.head_mouth(feat_mouth)\n",
    "        return logits_main, logits_mouth\n",
    "\n",
    "    def forward_main(self, x_full):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        return self.head_main(feat_full)\n",
    "\n",
    "\n",
    "# Load pre-trained backbone（SimCLR）\n",
    "base = efficientnet_b0(weights=None)\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "simclr_weights = torch.load(SIMCLR_PATH, map_location=\"cpu\")\n",
    "missing, unexpected = base.load_state_dict(simclr_weights, strict=False)\n",
    "print(\"Loaded SimCLR backbone. Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "model = FER5WithMouth(base, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# CE with class weight + label smoothing\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=class_weights_tensor,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Loss weights for the auxiliary head\n",
    "LAMBDA_MOUTH = 0.5\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "\n",
    "# 6. LR warmup + cosine decay\n",
    "total_steps  = EPOCHS * len(train_loader)\n",
    "warmup_ratio = 0.1\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return BASE_LR * float(step + 1) / float(warmup_steps + 1)\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    min_lr = BASE_LR * 0.1\n",
    "    return min_lr + (BASE_LR - min_lr) * cosine\n",
    "\n",
    "# 7. Training cycle\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    ce_main_sum = 0.0\n",
    "    ce_mouth_sum = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=100)\n",
    "    for imgs, labels in pbar:\n",
    "        # Original 7-class label -> Tensor\n",
    "        y_orig = torch.tensor(labels, dtype=torch.long)  # [B]\n",
    "        # Mapped to the new 5 categories\n",
    "        y_5 = mapping_5[y_orig]   # [B]\n",
    "        y_5 = y_5.to(device)\n",
    "\n",
    "        # Main branch input\n",
    "        x_full = torch.stack([train_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "        # Mouth branch input (first crop the mouth, then apply mouth augmentation)\n",
    "        mouth_imgs = [crop_mouth_pil(img) for img in imgs]\n",
    "        x_mouth = torch.stack([mouth_aug(m) for m in mouth_imgs]).to(device)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(global_step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Forward\n",
    "        logits_main, logits_mouth = model(x_full, x_mouth)\n",
    "\n",
    "        ce_main = criterion(logits_main, y_5)\n",
    "        ce_mouth = criterion(logits_mouth, y_5)\n",
    "\n",
    "        loss = ce_main + LAMBDA_MOUTH * ce_mouth\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        ce_main_sum += ce_main.item()\n",
    "        ce_mouth_sum += ce_mouth.item()\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"main\": f\"{ce_main.item():.4f}\",\n",
    "            \"mouth\": f\"{ce_mouth.item():.4f}\",\n",
    "            \"lr\":   f\"{lr:.1e}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_main = ce_main_sum / len(train_loader)\n",
    "    avg_mouth = ce_mouth_sum / len(train_loader)\n",
    "    print(f\"[Train] Epoch {epoch+1} | loss={avg_loss:.4f} | main={avg_main:.4f} | mouth={avg_mouth:.4f}\")\n",
    "\n",
    "    # Verification (main header only)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            y_orig = torch.tensor(labels, dtype=torch.long)\n",
    "            y_5 = mapping_5[y_orig].to(device)\n",
    "\n",
    "            x_val = torch.stack([val_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "            logits_val = model.forward_main(x_val)\n",
    "            loss_val = criterion(logits_val, y_5)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = logits_val.argmax(dim=1)\n",
    "            correct += (preds == y_5).sum().item()\n",
    "            total += len(y_5)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Val]   Epoch {epoch+1} | loss={val_loss:.4f} | acc={val_acc*100:.2f}%\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Saved model to {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21372828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train size (7-class): 10714, Val size: 3129\n",
      "Class index mapping: {'angry': 0, 'disgusted': 1, 'fearful': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprised': 6}\n",
      "7-class counts in train: Counter({6: 2248, 0: 2026, 2: 1922, 1: 1865, 5: 1582, 4: 744, 3: 327})\n",
      "Class weights (for CE): [0.4915323853492737, 0.5339649319648743, 0.5181293487548828, 3.045396327972412, 1.3385008573532104, 0.6294845938682556, 0.44299137592315674]\n",
      "Loading SimCLR weights from tinysimclr_effb0_mac.pth ...\n",
      "Loaded SimCLR backbone.\n",
      "Missing keys: ['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var', 'features.1.0.block.1.fc1.weight', 'features.1.0.block.1.fc1.bias', 'features.1.0.block.1.fc2.weight', 'features.1.0.block.1.fc2.bias', 'features.1.0.block.2.0.weight', 'features.1.0.block.2.1.weight', 'features.1.0.block.2.1.bias', 'features.1.0.block.2.1.running_mean', 'features.1.0.block.2.1.running_var', 'features.2.0.block.0.0.weight', 'features.2.0.block.0.1.weight', 'features.2.0.block.0.1.bias', 'features.2.0.block.0.1.running_mean', 'features.2.0.block.0.1.running_var', 'features.2.0.block.1.0.weight', 'features.2.0.block.1.1.weight', 'features.2.0.block.1.1.bias', 'features.2.0.block.1.1.running_mean', 'features.2.0.block.1.1.running_var', 'features.2.0.block.2.fc1.weight', 'features.2.0.block.2.fc1.bias', 'features.2.0.block.2.fc2.weight', 'features.2.0.block.2.fc2.bias', 'features.2.0.block.3.0.weight', 'features.2.0.block.3.1.weight', 'features.2.0.block.3.1.bias', 'features.2.0.block.3.1.running_mean', 'features.2.0.block.3.1.running_var', 'features.2.1.block.0.0.weight', 'features.2.1.block.0.1.weight', 'features.2.1.block.0.1.bias', 'features.2.1.block.0.1.running_mean', 'features.2.1.block.0.1.running_var', 'features.2.1.block.1.0.weight', 'features.2.1.block.1.1.weight', 'features.2.1.block.1.1.bias', 'features.2.1.block.1.1.running_mean', 'features.2.1.block.1.1.running_var', 'features.2.1.block.2.fc1.weight', 'features.2.1.block.2.fc1.bias', 'features.2.1.block.2.fc2.weight', 'features.2.1.block.2.fc2.bias', 'features.2.1.block.3.0.weight', 'features.2.1.block.3.1.weight', 'features.2.1.block.3.1.bias', 'features.2.1.block.3.1.running_mean', 'features.2.1.block.3.1.running_var', 'features.3.0.block.0.0.weight', 'features.3.0.block.0.1.weight', 'features.3.0.block.0.1.bias', 'features.3.0.block.0.1.running_mean', 'features.3.0.block.0.1.running_var', 'features.3.0.block.1.0.weight', 'features.3.0.block.1.1.weight', 'features.3.0.block.1.1.bias', 'features.3.0.block.1.1.running_mean', 'features.3.0.block.1.1.running_var', 'features.3.0.block.2.fc1.weight', 'features.3.0.block.2.fc1.bias', 'features.3.0.block.2.fc2.weight', 'features.3.0.block.2.fc2.bias', 'features.3.0.block.3.0.weight', 'features.3.0.block.3.1.weight', 'features.3.0.block.3.1.bias', 'features.3.0.block.3.1.running_mean', 'features.3.0.block.3.1.running_var', 'features.3.1.block.0.0.weight', 'features.3.1.block.0.1.weight', 'features.3.1.block.0.1.bias', 'features.3.1.block.0.1.running_mean', 'features.3.1.block.0.1.running_var', 'features.3.1.block.1.0.weight', 'features.3.1.block.1.1.weight', 'features.3.1.block.1.1.bias', 'features.3.1.block.1.1.running_mean', 'features.3.1.block.1.1.running_var', 'features.3.1.block.2.fc1.weight', 'features.3.1.block.2.fc1.bias', 'features.3.1.block.2.fc2.weight', 'features.3.1.block.2.fc2.bias', 'features.3.1.block.3.0.weight', 'features.3.1.block.3.1.weight', 'features.3.1.block.3.1.bias', 'features.3.1.block.3.1.running_mean', 'features.3.1.block.3.1.running_var', 'features.4.0.block.0.0.weight', 'features.4.0.block.0.1.weight', 'features.4.0.block.0.1.bias', 'features.4.0.block.0.1.running_mean', 'features.4.0.block.0.1.running_var', 'features.4.0.block.1.0.weight', 'features.4.0.block.1.1.weight', 'features.4.0.block.1.1.bias', 'features.4.0.block.1.1.running_mean', 'features.4.0.block.1.1.running_var', 'features.4.0.block.2.fc1.weight', 'features.4.0.block.2.fc1.bias', 'features.4.0.block.2.fc2.weight', 'features.4.0.block.2.fc2.bias', 'features.4.0.block.3.0.weight', 'features.4.0.block.3.1.weight', 'features.4.0.block.3.1.bias', 'features.4.0.block.3.1.running_mean', 'features.4.0.block.3.1.running_var', 'features.4.1.block.0.0.weight', 'features.4.1.block.0.1.weight', 'features.4.1.block.0.1.bias', 'features.4.1.block.0.1.running_mean', 'features.4.1.block.0.1.running_var', 'features.4.1.block.1.0.weight', 'features.4.1.block.1.1.weight', 'features.4.1.block.1.1.bias', 'features.4.1.block.1.1.running_mean', 'features.4.1.block.1.1.running_var', 'features.4.1.block.2.fc1.weight', 'features.4.1.block.2.fc1.bias', 'features.4.1.block.2.fc2.weight', 'features.4.1.block.2.fc2.bias', 'features.4.1.block.3.0.weight', 'features.4.1.block.3.1.weight', 'features.4.1.block.3.1.bias', 'features.4.1.block.3.1.running_mean', 'features.4.1.block.3.1.running_var', 'features.4.2.block.0.0.weight', 'features.4.2.block.0.1.weight', 'features.4.2.block.0.1.bias', 'features.4.2.block.0.1.running_mean', 'features.4.2.block.0.1.running_var', 'features.4.2.block.1.0.weight', 'features.4.2.block.1.1.weight', 'features.4.2.block.1.1.bias', 'features.4.2.block.1.1.running_mean', 'features.4.2.block.1.1.running_var', 'features.4.2.block.2.fc1.weight', 'features.4.2.block.2.fc1.bias', 'features.4.2.block.2.fc2.weight', 'features.4.2.block.2.fc2.bias', 'features.4.2.block.3.0.weight', 'features.4.2.block.3.1.weight', 'features.4.2.block.3.1.bias', 'features.4.2.block.3.1.running_mean', 'features.4.2.block.3.1.running_var', 'features.5.0.block.0.0.weight', 'features.5.0.block.0.1.weight', 'features.5.0.block.0.1.bias', 'features.5.0.block.0.1.running_mean', 'features.5.0.block.0.1.running_var', 'features.5.0.block.1.0.weight', 'features.5.0.block.1.1.weight', 'features.5.0.block.1.1.bias', 'features.5.0.block.1.1.running_mean', 'features.5.0.block.1.1.running_var', 'features.5.0.block.2.fc1.weight', 'features.5.0.block.2.fc1.bias', 'features.5.0.block.2.fc2.weight', 'features.5.0.block.2.fc2.bias', 'features.5.0.block.3.0.weight', 'features.5.0.block.3.1.weight', 'features.5.0.block.3.1.bias', 'features.5.0.block.3.1.running_mean', 'features.5.0.block.3.1.running_var', 'features.5.1.block.0.0.weight', 'features.5.1.block.0.1.weight', 'features.5.1.block.0.1.bias', 'features.5.1.block.0.1.running_mean', 'features.5.1.block.0.1.running_var', 'features.5.1.block.1.0.weight', 'features.5.1.block.1.1.weight', 'features.5.1.block.1.1.bias', 'features.5.1.block.1.1.running_mean', 'features.5.1.block.1.1.running_var', 'features.5.1.block.2.fc1.weight', 'features.5.1.block.2.fc1.bias', 'features.5.1.block.2.fc2.weight', 'features.5.1.block.2.fc2.bias', 'features.5.1.block.3.0.weight', 'features.5.1.block.3.1.weight', 'features.5.1.block.3.1.bias', 'features.5.1.block.3.1.running_mean', 'features.5.1.block.3.1.running_var', 'features.5.2.block.0.0.weight', 'features.5.2.block.0.1.weight', 'features.5.2.block.0.1.bias', 'features.5.2.block.0.1.running_mean', 'features.5.2.block.0.1.running_var', 'features.5.2.block.1.0.weight', 'features.5.2.block.1.1.weight', 'features.5.2.block.1.1.bias', 'features.5.2.block.1.1.running_mean', 'features.5.2.block.1.1.running_var', 'features.5.2.block.2.fc1.weight', 'features.5.2.block.2.fc1.bias', 'features.5.2.block.2.fc2.weight', 'features.5.2.block.2.fc2.bias', 'features.5.2.block.3.0.weight', 'features.5.2.block.3.1.weight', 'features.5.2.block.3.1.bias', 'features.5.2.block.3.1.running_mean', 'features.5.2.block.3.1.running_var', 'features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.0.1.running_mean', 'features.6.0.block.0.1.running_var', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.1.1.running_mean', 'features.6.0.block.1.1.running_var', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.0.block.3.1.running_mean', 'features.6.0.block.3.1.running_var', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.0.1.running_mean', 'features.6.1.block.0.1.running_var', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.1.1.running_mean', 'features.6.1.block.1.1.running_var', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.1.block.3.1.running_mean', 'features.6.1.block.3.1.running_var', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.0.1.running_mean', 'features.6.2.block.0.1.running_var', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.1.1.running_mean', 'features.6.2.block.1.1.running_var', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.2.block.3.1.running_mean', 'features.6.2.block.3.1.running_var', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.0.1.running_mean', 'features.6.3.block.0.1.running_var', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.1.1.running_mean', 'features.6.3.block.1.1.running_var', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.3.block.3.1.running_mean', 'features.6.3.block.3.1.running_var', 'features.7.0.block.0.0.weight', 'features.7.0.block.0.1.weight', 'features.7.0.block.0.1.bias', 'features.7.0.block.0.1.running_mean', 'features.7.0.block.0.1.running_var', 'features.7.0.block.1.0.weight', 'features.7.0.block.1.1.weight', 'features.7.0.block.1.1.bias', 'features.7.0.block.1.1.running_mean', 'features.7.0.block.1.1.running_var', 'features.7.0.block.2.fc1.weight', 'features.7.0.block.2.fc1.bias', 'features.7.0.block.2.fc2.weight', 'features.7.0.block.2.fc2.bias', 'features.7.0.block.3.0.weight', 'features.7.0.block.3.1.weight', 'features.7.0.block.3.1.bias', 'features.7.0.block.3.1.running_mean', 'features.7.0.block.3.1.running_var', 'features.8.0.weight', 'features.8.1.weight', 'features.8.1.bias', 'features.8.1.running_mean', 'features.8.1.running_var']\n",
      "Unexpected keys: ['encoder.features.0.0.weight', 'encoder.features.0.1.weight', 'encoder.features.0.1.bias', 'encoder.features.0.1.running_mean', 'encoder.features.0.1.running_var', 'encoder.features.0.1.num_batches_tracked', 'encoder.features.1.0.block.0.0.weight', 'encoder.features.1.0.block.0.1.weight', 'encoder.features.1.0.block.0.1.bias', 'encoder.features.1.0.block.0.1.running_mean', 'encoder.features.1.0.block.0.1.running_var', 'encoder.features.1.0.block.0.1.num_batches_tracked', 'encoder.features.1.0.block.1.fc1.weight', 'encoder.features.1.0.block.1.fc1.bias', 'encoder.features.1.0.block.1.fc2.weight', 'encoder.features.1.0.block.1.fc2.bias', 'encoder.features.1.0.block.2.0.weight', 'encoder.features.1.0.block.2.1.weight', 'encoder.features.1.0.block.2.1.bias', 'encoder.features.1.0.block.2.1.running_mean', 'encoder.features.1.0.block.2.1.running_var', 'encoder.features.1.0.block.2.1.num_batches_tracked', 'encoder.features.2.0.block.0.0.weight', 'encoder.features.2.0.block.0.1.weight', 'encoder.features.2.0.block.0.1.bias', 'encoder.features.2.0.block.0.1.running_mean', 'encoder.features.2.0.block.0.1.running_var', 'encoder.features.2.0.block.0.1.num_batches_tracked', 'encoder.features.2.0.block.1.0.weight', 'encoder.features.2.0.block.1.1.weight', 'encoder.features.2.0.block.1.1.bias', 'encoder.features.2.0.block.1.1.running_mean', 'encoder.features.2.0.block.1.1.running_var', 'encoder.features.2.0.block.1.1.num_batches_tracked', 'encoder.features.2.0.block.2.fc1.weight', 'encoder.features.2.0.block.2.fc1.bias', 'encoder.features.2.0.block.2.fc2.weight', 'encoder.features.2.0.block.2.fc2.bias', 'encoder.features.2.0.block.3.0.weight', 'encoder.features.2.0.block.3.1.weight', 'encoder.features.2.0.block.3.1.bias', 'encoder.features.2.0.block.3.1.running_mean', 'encoder.features.2.0.block.3.1.running_var', 'encoder.features.2.0.block.3.1.num_batches_tracked', 'encoder.features.2.1.block.0.0.weight', 'encoder.features.2.1.block.0.1.weight', 'encoder.features.2.1.block.0.1.bias', 'encoder.features.2.1.block.0.1.running_mean', 'encoder.features.2.1.block.0.1.running_var', 'encoder.features.2.1.block.0.1.num_batches_tracked', 'encoder.features.2.1.block.1.0.weight', 'encoder.features.2.1.block.1.1.weight', 'encoder.features.2.1.block.1.1.bias', 'encoder.features.2.1.block.1.1.running_mean', 'encoder.features.2.1.block.1.1.running_var', 'encoder.features.2.1.block.1.1.num_batches_tracked', 'encoder.features.2.1.block.2.fc1.weight', 'encoder.features.2.1.block.2.fc1.bias', 'encoder.features.2.1.block.2.fc2.weight', 'encoder.features.2.1.block.2.fc2.bias', 'encoder.features.2.1.block.3.0.weight', 'encoder.features.2.1.block.3.1.weight', 'encoder.features.2.1.block.3.1.bias', 'encoder.features.2.1.block.3.1.running_mean', 'encoder.features.2.1.block.3.1.running_var', 'encoder.features.2.1.block.3.1.num_batches_tracked', 'encoder.features.3.0.block.0.0.weight', 'encoder.features.3.0.block.0.1.weight', 'encoder.features.3.0.block.0.1.bias', 'encoder.features.3.0.block.0.1.running_mean', 'encoder.features.3.0.block.0.1.running_var', 'encoder.features.3.0.block.0.1.num_batches_tracked', 'encoder.features.3.0.block.1.0.weight', 'encoder.features.3.0.block.1.1.weight', 'encoder.features.3.0.block.1.1.bias', 'encoder.features.3.0.block.1.1.running_mean', 'encoder.features.3.0.block.1.1.running_var', 'encoder.features.3.0.block.1.1.num_batches_tracked', 'encoder.features.3.0.block.2.fc1.weight', 'encoder.features.3.0.block.2.fc1.bias', 'encoder.features.3.0.block.2.fc2.weight', 'encoder.features.3.0.block.2.fc2.bias', 'encoder.features.3.0.block.3.0.weight', 'encoder.features.3.0.block.3.1.weight', 'encoder.features.3.0.block.3.1.bias', 'encoder.features.3.0.block.3.1.running_mean', 'encoder.features.3.0.block.3.1.running_var', 'encoder.features.3.0.block.3.1.num_batches_tracked', 'encoder.features.3.1.block.0.0.weight', 'encoder.features.3.1.block.0.1.weight', 'encoder.features.3.1.block.0.1.bias', 'encoder.features.3.1.block.0.1.running_mean', 'encoder.features.3.1.block.0.1.running_var', 'encoder.features.3.1.block.0.1.num_batches_tracked', 'encoder.features.3.1.block.1.0.weight', 'encoder.features.3.1.block.1.1.weight', 'encoder.features.3.1.block.1.1.bias', 'encoder.features.3.1.block.1.1.running_mean', 'encoder.features.3.1.block.1.1.running_var', 'encoder.features.3.1.block.1.1.num_batches_tracked', 'encoder.features.3.1.block.2.fc1.weight', 'encoder.features.3.1.block.2.fc1.bias', 'encoder.features.3.1.block.2.fc2.weight', 'encoder.features.3.1.block.2.fc2.bias', 'encoder.features.3.1.block.3.0.weight', 'encoder.features.3.1.block.3.1.weight', 'encoder.features.3.1.block.3.1.bias', 'encoder.features.3.1.block.3.1.running_mean', 'encoder.features.3.1.block.3.1.running_var', 'encoder.features.3.1.block.3.1.num_batches_tracked', 'encoder.features.4.0.block.0.0.weight', 'encoder.features.4.0.block.0.1.weight', 'encoder.features.4.0.block.0.1.bias', 'encoder.features.4.0.block.0.1.running_mean', 'encoder.features.4.0.block.0.1.running_var', 'encoder.features.4.0.block.0.1.num_batches_tracked', 'encoder.features.4.0.block.1.0.weight', 'encoder.features.4.0.block.1.1.weight', 'encoder.features.4.0.block.1.1.bias', 'encoder.features.4.0.block.1.1.running_mean', 'encoder.features.4.0.block.1.1.running_var', 'encoder.features.4.0.block.1.1.num_batches_tracked', 'encoder.features.4.0.block.2.fc1.weight', 'encoder.features.4.0.block.2.fc1.bias', 'encoder.features.4.0.block.2.fc2.weight', 'encoder.features.4.0.block.2.fc2.bias', 'encoder.features.4.0.block.3.0.weight', 'encoder.features.4.0.block.3.1.weight', 'encoder.features.4.0.block.3.1.bias', 'encoder.features.4.0.block.3.1.running_mean', 'encoder.features.4.0.block.3.1.running_var', 'encoder.features.4.0.block.3.1.num_batches_tracked', 'encoder.features.4.1.block.0.0.weight', 'encoder.features.4.1.block.0.1.weight', 'encoder.features.4.1.block.0.1.bias', 'encoder.features.4.1.block.0.1.running_mean', 'encoder.features.4.1.block.0.1.running_var', 'encoder.features.4.1.block.0.1.num_batches_tracked', 'encoder.features.4.1.block.1.0.weight', 'encoder.features.4.1.block.1.1.weight', 'encoder.features.4.1.block.1.1.bias', 'encoder.features.4.1.block.1.1.running_mean', 'encoder.features.4.1.block.1.1.running_var', 'encoder.features.4.1.block.1.1.num_batches_tracked', 'encoder.features.4.1.block.2.fc1.weight', 'encoder.features.4.1.block.2.fc1.bias', 'encoder.features.4.1.block.2.fc2.weight', 'encoder.features.4.1.block.2.fc2.bias', 'encoder.features.4.1.block.3.0.weight', 'encoder.features.4.1.block.3.1.weight', 'encoder.features.4.1.block.3.1.bias', 'encoder.features.4.1.block.3.1.running_mean', 'encoder.features.4.1.block.3.1.running_var', 'encoder.features.4.1.block.3.1.num_batches_tracked', 'encoder.features.4.2.block.0.0.weight', 'encoder.features.4.2.block.0.1.weight', 'encoder.features.4.2.block.0.1.bias', 'encoder.features.4.2.block.0.1.running_mean', 'encoder.features.4.2.block.0.1.running_var', 'encoder.features.4.2.block.0.1.num_batches_tracked', 'encoder.features.4.2.block.1.0.weight', 'encoder.features.4.2.block.1.1.weight', 'encoder.features.4.2.block.1.1.bias', 'encoder.features.4.2.block.1.1.running_mean', 'encoder.features.4.2.block.1.1.running_var', 'encoder.features.4.2.block.1.1.num_batches_tracked', 'encoder.features.4.2.block.2.fc1.weight', 'encoder.features.4.2.block.2.fc1.bias', 'encoder.features.4.2.block.2.fc2.weight', 'encoder.features.4.2.block.2.fc2.bias', 'encoder.features.4.2.block.3.0.weight', 'encoder.features.4.2.block.3.1.weight', 'encoder.features.4.2.block.3.1.bias', 'encoder.features.4.2.block.3.1.running_mean', 'encoder.features.4.2.block.3.1.running_var', 'encoder.features.4.2.block.3.1.num_batches_tracked', 'encoder.features.5.0.block.0.0.weight', 'encoder.features.5.0.block.0.1.weight', 'encoder.features.5.0.block.0.1.bias', 'encoder.features.5.0.block.0.1.running_mean', 'encoder.features.5.0.block.0.1.running_var', 'encoder.features.5.0.block.0.1.num_batches_tracked', 'encoder.features.5.0.block.1.0.weight', 'encoder.features.5.0.block.1.1.weight', 'encoder.features.5.0.block.1.1.bias', 'encoder.features.5.0.block.1.1.running_mean', 'encoder.features.5.0.block.1.1.running_var', 'encoder.features.5.0.block.1.1.num_batches_tracked', 'encoder.features.5.0.block.2.fc1.weight', 'encoder.features.5.0.block.2.fc1.bias', 'encoder.features.5.0.block.2.fc2.weight', 'encoder.features.5.0.block.2.fc2.bias', 'encoder.features.5.0.block.3.0.weight', 'encoder.features.5.0.block.3.1.weight', 'encoder.features.5.0.block.3.1.bias', 'encoder.features.5.0.block.3.1.running_mean', 'encoder.features.5.0.block.3.1.running_var', 'encoder.features.5.0.block.3.1.num_batches_tracked', 'encoder.features.5.1.block.0.0.weight', 'encoder.features.5.1.block.0.1.weight', 'encoder.features.5.1.block.0.1.bias', 'encoder.features.5.1.block.0.1.running_mean', 'encoder.features.5.1.block.0.1.running_var', 'encoder.features.5.1.block.0.1.num_batches_tracked', 'encoder.features.5.1.block.1.0.weight', 'encoder.features.5.1.block.1.1.weight', 'encoder.features.5.1.block.1.1.bias', 'encoder.features.5.1.block.1.1.running_mean', 'encoder.features.5.1.block.1.1.running_var', 'encoder.features.5.1.block.1.1.num_batches_tracked', 'encoder.features.5.1.block.2.fc1.weight', 'encoder.features.5.1.block.2.fc1.bias', 'encoder.features.5.1.block.2.fc2.weight', 'encoder.features.5.1.block.2.fc2.bias', 'encoder.features.5.1.block.3.0.weight', 'encoder.features.5.1.block.3.1.weight', 'encoder.features.5.1.block.3.1.bias', 'encoder.features.5.1.block.3.1.running_mean', 'encoder.features.5.1.block.3.1.running_var', 'encoder.features.5.1.block.3.1.num_batches_tracked', 'encoder.features.5.2.block.0.0.weight', 'encoder.features.5.2.block.0.1.weight', 'encoder.features.5.2.block.0.1.bias', 'encoder.features.5.2.block.0.1.running_mean', 'encoder.features.5.2.block.0.1.running_var', 'encoder.features.5.2.block.0.1.num_batches_tracked', 'encoder.features.5.2.block.1.0.weight', 'encoder.features.5.2.block.1.1.weight', 'encoder.features.5.2.block.1.1.bias', 'encoder.features.5.2.block.1.1.running_mean', 'encoder.features.5.2.block.1.1.running_var', 'encoder.features.5.2.block.1.1.num_batches_tracked', 'encoder.features.5.2.block.2.fc1.weight', 'encoder.features.5.2.block.2.fc1.bias', 'encoder.features.5.2.block.2.fc2.weight', 'encoder.features.5.2.block.2.fc2.bias', 'encoder.features.5.2.block.3.0.weight', 'encoder.features.5.2.block.3.1.weight', 'encoder.features.5.2.block.3.1.bias', 'encoder.features.5.2.block.3.1.running_mean', 'encoder.features.5.2.block.3.1.running_var', 'encoder.features.5.2.block.3.1.num_batches_tracked', 'encoder.features.6.0.block.0.0.weight', 'encoder.features.6.0.block.0.1.weight', 'encoder.features.6.0.block.0.1.bias', 'encoder.features.6.0.block.0.1.running_mean', 'encoder.features.6.0.block.0.1.running_var', 'encoder.features.6.0.block.0.1.num_batches_tracked', 'encoder.features.6.0.block.1.0.weight', 'encoder.features.6.0.block.1.1.weight', 'encoder.features.6.0.block.1.1.bias', 'encoder.features.6.0.block.1.1.running_mean', 'encoder.features.6.0.block.1.1.running_var', 'encoder.features.6.0.block.1.1.num_batches_tracked', 'encoder.features.6.0.block.2.fc1.weight', 'encoder.features.6.0.block.2.fc1.bias', 'encoder.features.6.0.block.2.fc2.weight', 'encoder.features.6.0.block.2.fc2.bias', 'encoder.features.6.0.block.3.0.weight', 'encoder.features.6.0.block.3.1.weight', 'encoder.features.6.0.block.3.1.bias', 'encoder.features.6.0.block.3.1.running_mean', 'encoder.features.6.0.block.3.1.running_var', 'encoder.features.6.0.block.3.1.num_batches_tracked', 'encoder.features.6.1.block.0.0.weight', 'encoder.features.6.1.block.0.1.weight', 'encoder.features.6.1.block.0.1.bias', 'encoder.features.6.1.block.0.1.running_mean', 'encoder.features.6.1.block.0.1.running_var', 'encoder.features.6.1.block.0.1.num_batches_tracked', 'encoder.features.6.1.block.1.0.weight', 'encoder.features.6.1.block.1.1.weight', 'encoder.features.6.1.block.1.1.bias', 'encoder.features.6.1.block.1.1.running_mean', 'encoder.features.6.1.block.1.1.running_var', 'encoder.features.6.1.block.1.1.num_batches_tracked', 'encoder.features.6.1.block.2.fc1.weight', 'encoder.features.6.1.block.2.fc1.bias', 'encoder.features.6.1.block.2.fc2.weight', 'encoder.features.6.1.block.2.fc2.bias', 'encoder.features.6.1.block.3.0.weight', 'encoder.features.6.1.block.3.1.weight', 'encoder.features.6.1.block.3.1.bias', 'encoder.features.6.1.block.3.1.running_mean', 'encoder.features.6.1.block.3.1.running_var', 'encoder.features.6.1.block.3.1.num_batches_tracked', 'encoder.features.6.2.block.0.0.weight', 'encoder.features.6.2.block.0.1.weight', 'encoder.features.6.2.block.0.1.bias', 'encoder.features.6.2.block.0.1.running_mean', 'encoder.features.6.2.block.0.1.running_var', 'encoder.features.6.2.block.0.1.num_batches_tracked', 'encoder.features.6.2.block.1.0.weight', 'encoder.features.6.2.block.1.1.weight', 'encoder.features.6.2.block.1.1.bias', 'encoder.features.6.2.block.1.1.running_mean', 'encoder.features.6.2.block.1.1.running_var', 'encoder.features.6.2.block.1.1.num_batches_tracked', 'encoder.features.6.2.block.2.fc1.weight', 'encoder.features.6.2.block.2.fc1.bias', 'encoder.features.6.2.block.2.fc2.weight', 'encoder.features.6.2.block.2.fc2.bias', 'encoder.features.6.2.block.3.0.weight', 'encoder.features.6.2.block.3.1.weight', 'encoder.features.6.2.block.3.1.bias', 'encoder.features.6.2.block.3.1.running_mean', 'encoder.features.6.2.block.3.1.running_var', 'encoder.features.6.2.block.3.1.num_batches_tracked', 'encoder.features.6.3.block.0.0.weight', 'encoder.features.6.3.block.0.1.weight', 'encoder.features.6.3.block.0.1.bias', 'encoder.features.6.3.block.0.1.running_mean', 'encoder.features.6.3.block.0.1.running_var', 'encoder.features.6.3.block.0.1.num_batches_tracked', 'encoder.features.6.3.block.1.0.weight', 'encoder.features.6.3.block.1.1.weight', 'encoder.features.6.3.block.1.1.bias', 'encoder.features.6.3.block.1.1.running_mean', 'encoder.features.6.3.block.1.1.running_var', 'encoder.features.6.3.block.1.1.num_batches_tracked', 'encoder.features.6.3.block.2.fc1.weight', 'encoder.features.6.3.block.2.fc1.bias', 'encoder.features.6.3.block.2.fc2.weight', 'encoder.features.6.3.block.2.fc2.bias', 'encoder.features.6.3.block.3.0.weight', 'encoder.features.6.3.block.3.1.weight', 'encoder.features.6.3.block.3.1.bias', 'encoder.features.6.3.block.3.1.running_mean', 'encoder.features.6.3.block.3.1.running_var', 'encoder.features.6.3.block.3.1.num_batches_tracked', 'encoder.features.7.0.block.0.0.weight', 'encoder.features.7.0.block.0.1.weight', 'encoder.features.7.0.block.0.1.bias', 'encoder.features.7.0.block.0.1.running_mean', 'encoder.features.7.0.block.0.1.running_var', 'encoder.features.7.0.block.0.1.num_batches_tracked', 'encoder.features.7.0.block.1.0.weight', 'encoder.features.7.0.block.1.1.weight', 'encoder.features.7.0.block.1.1.bias', 'encoder.features.7.0.block.1.1.running_mean', 'encoder.features.7.0.block.1.1.running_var', 'encoder.features.7.0.block.1.1.num_batches_tracked', 'encoder.features.7.0.block.2.fc1.weight', 'encoder.features.7.0.block.2.fc1.bias', 'encoder.features.7.0.block.2.fc2.weight', 'encoder.features.7.0.block.2.fc2.bias', 'encoder.features.7.0.block.3.0.weight', 'encoder.features.7.0.block.3.1.weight', 'encoder.features.7.0.block.3.1.bias', 'encoder.features.7.0.block.3.1.running_mean', 'encoder.features.7.0.block.3.1.running_var', 'encoder.features.7.0.block.3.1.num_batches_tracked', 'encoder.features.8.0.weight', 'encoder.features.8.1.weight', 'encoder.features.8.1.bias', 'encoder.features.8.1.running_mean', 'encoder.features.8.1.running_var', 'encoder.features.8.1.num_batches_tracked', 'projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█| 670/670 [05:54<00:00,  1.89it/s, loss=2.5194, main=1.6410, mouth=1.7567, lr=1.2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 | loss=2.6546 | main=1.7679 | mouth=1.7733\n",
      "[Val]   Epoch 1 | loss=2.6095 | acc=3.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=2.4057, main=1.5473, mouth=1.7167, lr=2.4e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 | loss=2.5659 | main=1.7102 | mouth=1.7114\n",
      "[Val]   Epoch 2 | loss=3.2005 | acc=3.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=1.5711, main=1.0384, mouth=1.0656, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3 | loss=2.4401 | main=1.6169 | mouth=1.6463\n",
      "[Val]   Epoch 3 | loss=2.4019 | acc=10.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|█| 670/670 [05:24<00:00,  2.07it/s, loss=1.2669, main=0.7880, mouth=0.9578, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4 | loss=2.2365 | main=1.4712 | mouth=1.5307\n",
      "[Val]   Epoch 4 | loss=2.3826 | acc=11.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|█| 670/670 [05:35<00:00,  2.00it/s, loss=2.2532, main=1.5069, mouth=1.4927, lr=2.9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5 | loss=2.0434 | main=1.3310 | mouth=1.4247\n",
      "[Val]   Epoch 5 | loss=2.2386 | acc=22.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|█| 670/670 [05:33<00:00,  2.01it/s, loss=1.7423, main=1.1333, mouth=1.2180, lr=2.8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6 | loss=1.9054 | main=1.2270 | mouth=1.3568\n",
      "[Val]   Epoch 6 | loss=2.1749 | acc=29.56%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=1.9278, main=1.2412, mouth=1.3731, lr=2.7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7 | loss=1.8507 | main=1.1800 | mouth=1.3414\n",
      "[Val]   Epoch 7 | loss=2.0285 | acc=31.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|█| 670/670 [05:32<00:00,  2.01it/s, loss=2.0048, main=1.2626, mouth=1.4845, lr=2.6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8 | loss=1.7812 | main=1.1286 | mouth=1.3052\n",
      "[Val]   Epoch 8 | loss=2.1659 | acc=31.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=1.8605, main=1.2075, mouth=1.3059, lr=2.5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9 | loss=1.6461 | main=1.0354 | mouth=1.2213\n",
      "[Val]   Epoch 9 | loss=1.9659 | acc=38.80%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.0644, main=0.6607, mouth=0.8074, lr=2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10 | loss=1.6335 | main=1.0187 | mouth=1.2295\n",
      "[Val]   Epoch 10 | loss=1.8887 | acc=41.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|█| 670/670 [05:31<00:00,  2.02it/s, loss=0.9832, main=0.6379, mouth=0.6906, lr=2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11 | loss=1.5610 | main=0.9680 | mouth=1.1861\n",
      "[Val]   Epoch 11 | loss=1.8435 | acc=45.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.5813, main=0.9896, mouth=1.1833, lr=2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12 | loss=1.5407 | main=0.9523 | mouth=1.1767\n",
      "[Val]   Epoch 12 | loss=1.8043 | acc=48.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=2.5062, main=1.5159, mouth=1.9805, lr=1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13 | loss=1.4655 | main=0.8929 | mouth=1.1451\n",
      "[Val]   Epoch 13 | loss=1.7486 | acc=50.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|█| 670/670 [05:32<00:00,  2.02it/s, loss=2.1882, main=1.3564, mouth=1.6635, lr=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14 | loss=1.4425 | main=0.8763 | mouth=1.1324\n",
      "[Val]   Epoch 14 | loss=1.7682 | acc=51.55%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.0975, main=0.6789, mouth=0.8372, lr=1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15 | loss=1.3624 | main=0.8229 | mouth=1.0790\n",
      "[Val]   Epoch 15 | loss=1.7573 | acc=51.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|█| 670/670 [05:21<00:00,  2.08it/s, loss=0.8268, main=0.4247, mouth=0.8042, lr=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16 | loss=1.3509 | main=0.8108 | mouth=1.0803\n",
      "[Val]   Epoch 16 | loss=1.6413 | acc=55.61%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|█| 670/670 [05:21<00:00,  2.08it/s, loss=0.7347, main=0.3320, mouth=0.8054, lr=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17 | loss=1.2931 | main=0.7679 | mouth=1.0505\n",
      "[Val]   Epoch 17 | loss=1.6776 | acc=55.54%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|█| 670/670 [05:44<00:00,  1.95it/s, loss=1.8305, main=1.2329, mouth=1.1952, lr=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18 | loss=1.2612 | main=0.7497 | mouth=1.0230\n",
      "[Val]   Epoch 18 | loss=1.6417 | acc=55.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|█| 670/670 [07:38<00:00,  1.46it/s, loss=1.4571, main=0.8050, mouth=1.3042, lr=7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19 | loss=1.2527 | main=0.7397 | mouth=1.0260\n",
      "[Val]   Epoch 19 | loss=1.6179 | acc=57.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|█| 670/670 [05:38<00:00,  1.98it/s, loss=1.0161, main=0.6445, mouth=0.7432, lr=6.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20 | loss=1.1929 | main=0.7005 | mouth=0.9848\n",
      "[Val]   Epoch 20 | loss=1.6342 | acc=57.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|█| 670/670 [05:46<00:00,  1.93it/s, loss=1.7307, main=1.1403, mouth=1.1809, lr=5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21 | loss=1.1764 | main=0.6894 | mouth=0.9739\n",
      "[Val]   Epoch 21 | loss=1.6027 | acc=59.57%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|█| 670/670 [05:48<00:00,  1.92it/s, loss=0.9307, main=0.3804, mouth=1.1007, lr=4.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22 | loss=1.1700 | main=0.6857 | mouth=0.9686\n",
      "[Val]   Epoch 22 | loss=1.6086 | acc=58.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|█| 670/670 [05:47<00:00,  1.93it/s, loss=1.1268, main=0.6671, mouth=0.9193, lr=3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23 | loss=1.1929 | main=0.6981 | mouth=0.9896\n",
      "[Val]   Epoch 23 | loss=1.5946 | acc=58.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|█| 670/670 [05:43<00:00,  1.95it/s, loss=1.0567, main=0.6026, mouth=0.9082, lr=3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24 | loss=1.1559 | main=0.6752 | mouth=0.9614\n",
      "[Val]   Epoch 24 | loss=1.6238 | acc=58.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|█| 670/670 [05:33<00:00,  2.01it/s, loss=0.8793, main=0.4335, mouth=0.8915, lr=3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25 | loss=1.1193 | main=0.6544 | mouth=0.9299\n",
      "[Val]   Epoch 25 | loss=1.6099 | acc=59.83%\n",
      "\n",
      "Saved model to affectnet_7cls_mouth.pth\n"
     ]
    }
   ],
   "source": [
    "# fine tuning v3(back to 7 classes)\n",
    "# fine_tune_7cls_mouth.py\n",
    "# 7 classes + SimCLR backbone + class weight + balanced sampler + mouth auxiliary head\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT   = \"YOLO_format_cls\"\n",
    "TRAIN_DIR   = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR     = os.path.join(DATA_ROOT, \"valid\")\n",
    "\n",
    "SIMCLR_PATH = \"tinysimclr_effb0_mac.pth\"   # Previously trained TinySimCLR weights\n",
    "SAVE_PATH   = \"affectnet_7cls_mouth.pth\"   # Output model filename\n",
    "\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 25\n",
    "BASE_LR     = 3e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# 1. collate_fn: Preserve PIL image\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)   # imgs: list[PIL.Image], labels: list[int]\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "\n",
    "# 2. Data Augmentation (Real-time Scenario-Friendly Edition)\n",
    "# Primary branch: Mild rotation + Mirroring\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Mouth branch: Apply a similar enhancement to the mouth crop.\n",
    "mouth_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Verification: Resize only + normalisation\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Simple 'mouth crop' function\n",
    "def crop_mouth_pil(img):\n",
    "    \"\"\"\n",
    "    Non-critical point version: Directly take the lower half of the face and treat it as the \"mouth region\".\n",
    "    img: PIL.Image\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    top = int(h * 0.55)  # Discard the top 55%, retaining the bottom 45%.\n",
    "    return img.crop((0, top, w, h))  # (left, upper, right, lower)\n",
    "\n",
    "\n",
    "# 4. Dataset + Category Statistics + Balanced Sampler + Class Weight\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=None)\n",
    "val_set   = datasets.ImageFolder(VAL_DIR, transform=None)\n",
    "\n",
    "print(f\"Train size (7-class): {len(train_set)}, Val size: {len(val_set)}\")\n",
    "print(\"Class index mapping:\", train_set.class_to_idx)  # Observe the sequence of angry/disgust/...\n",
    "\n",
    "# ImageFolder will store all labels train_set.targets（list[int]）\n",
    "orig_targets = train_set.targets\n",
    "\n",
    "# Count the number of samples in each category\n",
    "counter_7 = Counter(orig_targets)\n",
    "print(\"7-class counts in train:\", counter_7)\n",
    "\n",
    "# Calculate class weight: 1 / count Then normalise\n",
    "class_weights = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    count_c = counter_7.get(c, 1)  # Theoretically, it cannot be zero.\n",
    "    class_weights.append(1.0 / count_c)\n",
    "\n",
    "sum_w = sum(class_weights)\n",
    "class_weights = [w / sum_w * NUM_CLASSES for w in class_weights]\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights (for CE):\", class_weights_tensor.tolist())\n",
    "\n",
    "# Balanced Sampler（WeightedRandomSampler\n",
    "sample_weights = [class_weights[label] for label in orig_targets]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Backbone + Mouth auxiliary head model\n",
    "class FER7WithMouth(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-B0 backbone + Primary Head (Full Face) + Secondary Head (Mouth)\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone  # EfficientNet-B0, classifier = Identity（output 1280）\n",
    "\n",
    "        # Main Head: Full FaceMain Head: Full Face\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Auxiliary head: Mouth\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        feat_full = self.backbone(x_full)    # [B, 1280]\n",
    "        feat_mouth = self.backbone(x_mouth)  # [B, 1280]（Weight sharing\n",
    "\n",
    "        logits_main = self.head_main(feat_full)\n",
    "        logits_mouth = self.head_mouth(feat_mouth)\n",
    "        return logits_main, logits_mouth\n",
    "\n",
    "    def forward_main(self, x_full):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        return self.head_main(feat_full)\n",
    "\n",
    "\n",
    "# Loading SimCLR pre-trained backbone\n",
    "base = efficientnet_b0(weights=None)\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "print(f\"Loading SimCLR weights from {SIMCLR_PATH} ...\")\n",
    "simclr_weights = torch.load(SIMCLR_PATH, map_location=\"cpu\")\n",
    "missing, unexpected = base.load_state_dict(simclr_weights, strict=False)\n",
    "print(\"Loaded SimCLR backbone.\")\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "model = FER7WithMouth(base, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# CE with class weight + label smoothing\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=class_weights_tensor,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Weight for auxiliary head loss\n",
    "LAMBDA_MOUTH = 0.5\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "\n",
    "# 6. LR warmup + cosine decay\n",
    "total_steps  = EPOCHS * len(train_loader)\n",
    "warmup_ratio = 0.1\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup: 0 to BASE_LR\n",
    "        return BASE_LR * float(step + 1) / float(warmup_steps + 1)\n",
    "    # Subsequently, the cosine decays to BASE_LR multiplied by 0.1.\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    min_lr = BASE_LR * 0.1\n",
    "    return min_lr + (BASE_LR - min_lr) * cosine\n",
    "\n",
    "# 7. Training cycle\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    ce_main_sum = 0.0\n",
    "    ce_mouth_sum = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=100)\n",
    "    for imgs, labels in pbar:\n",
    "        # labels: list[int]，0~6\n",
    "        y = torch.tensor(labels, dtype=torch.long).to(device)  # [B]\n",
    "\n",
    "        # Main branch (full face)\n",
    "        x_full = torch.stack([train_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "        # mouth branch（mouth dection + mouth_aug）\n",
    "        mouth_imgs = [crop_mouth_pil(img) for img in imgs]\n",
    "        x_mouth = torch.stack([mouth_aug(m) for m in mouth_imgs]).to(device)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(global_step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "        # Forward\n",
    "        logits_main, logits_mouth = model(x_full, x_mouth)\n",
    "\n",
    "        ce_main = criterion(logits_main, y)\n",
    "        ce_mouth = criterion(logits_mouth, y)\n",
    "\n",
    "        loss = ce_main + LAMBDA_MOUTH * ce_mouth\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient cropping, steady now\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        ce_main_sum += ce_main.item()\n",
    "        ce_mouth_sum += ce_mouth.item()\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\":  f\"{loss.item():.4f}\",\n",
    "            \"main\":  f\"{ce_main.item():.4f}\",\n",
    "            \"mouth\": f\"{ce_mouth.item():.4f}\",\n",
    "            \"lr\":    f\"{lr:.1e}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_main = ce_main_sum / len(train_loader)\n",
    "    avg_mouth = ce_mouth_sum / len(train_loader)\n",
    "    print(f\"[Train] Epoch {epoch+1} | loss={avg_loss:.4f} | main={avg_main:.4f} | mouth={avg_mouth:.4f}\")\n",
    "\n",
    "    # 8. Verification (main header only)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            y_val = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "            x_val = torch.stack([val_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "            logits_val = model.forward_main(x_val)\n",
    "            loss_val = criterion(logits_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = logits_val.argmax(dim=1)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "            total += len(y_val)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Val]   Epoch {epoch+1} | loss={val_loss:.4f} | acc={val_acc*100:.2f}%\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Saved model to {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2e0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxscript\n",
      "  Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: ml_dtypes in /Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages (from onnxscript) (0.5.3)\n",
      "Requirement already satisfied: numpy in /Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages (from onnxscript) (1.24.3)\n",
      "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
      "  Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting onnx>=1.16 (from onnxscript)\n",
      "  Downloading onnx-1.19.1-cp310-cp310-macosx_12_0_universal2.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: packaging in /Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages (from onnxscript) (25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages (from onnxscript) (4.15.0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages (from onnx>=1.16->onnxscript) (4.25.8)\n",
      "Downloading onnxscript-0.5.6-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.0/683.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)\n",
      "Downloading onnx-1.19.1-cp310-cp310-macosx_12_0_universal2.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx, onnx_ir, onnxscript\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [onnxscript]3\u001b[0m [onnxscript]\n",
      "\u001b[1A\u001b[2KSuccessfully installed onnx-1.19.1 onnx_ir-0.1.12 onnxscript-0.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d83740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from affectnet_7cls_mouth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n1/34sz7g4n08jg7s8blpt80cqc0000gn/T/ipykernel_24146/152223584.py:62: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1117 14:48:59.735000 24146 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 12 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FER7WithMouth([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FER7WithMouth([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 12).\n",
      "Failed to convert the model to the target version 12 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 147 of general pattern rewrite rules.\n",
      "Exported full ONNX model to affectnet_7cls_mouth_full.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "class FER7WithMouth(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        feat_mouth = self.backbone(x_mouth)\n",
    "        return self.head_main(feat_full), self.head_mouth(feat_mouth)\n",
    "\n",
    "MODEL_PATH = \"affectnet_7cls_mouth.pth\"\n",
    "ONNX_PATH  = \"affectnet_7cls_mouth_full.onnx\"\n",
    "\n",
    "base = efficientnet_b0(weights=None)\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "model = FER7WithMouth(base, num_classes=7)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model from\", MODEL_PATH)\n",
    "\n",
    "dummy_full  = torch.randn(1, 3, 224, 224)\n",
    "dummy_mouth = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_full, dummy_mouth),\n",
    "    ONNX_PATH,\n",
    "    input_names=[\"full_input\", \"mouth_input\"],\n",
    "    output_names=[\"main_logits\", \"mouth_logits\"],\n",
    "    opset_version=12,\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        \"full_input\": {0: \"batch\"},\n",
    "        \"mouth_input\": {0: \"batch\"},\n",
    "        \"main_logits\": {0: \"batch\"},\n",
    "        \"mouth_logits\": {0: \"batch\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Exported full ONNX model to\", ONNX_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ede22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train size (7-class): 10714, Val size: 3129\n",
      "Class index mapping: {'angry': 0, 'disgusted': 1, 'fearful': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprised': 6}\n",
      "7-class counts in train: Counter({6: 2248, 0: 2026, 2: 1922, 1: 1865, 5: 1582, 4: 744, 3: 327})\n",
      "Class weights (for CE, after angry boost): [1.2119460105895996, 0.8777132034301758, 0.4258415997028351, 2.5029587745666504, 1.100090742111206, 0.5173625349998474, 0.36408698558807373]\n",
      "Loading SimCLR weights from tinysimclr_effb0_mac.pth ...\n",
      "Loaded SimCLR backbone.\n",
      "Missing keys: ['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var', 'features.1.0.block.1.fc1.weight', 'features.1.0.block.1.fc1.bias', 'features.1.0.block.1.fc2.weight', 'features.1.0.block.1.fc2.bias', 'features.1.0.block.2.0.weight', 'features.1.0.block.2.1.weight', 'features.1.0.block.2.1.bias', 'features.1.0.block.2.1.running_mean', 'features.1.0.block.2.1.running_var', 'features.2.0.block.0.0.weight', 'features.2.0.block.0.1.weight', 'features.2.0.block.0.1.bias', 'features.2.0.block.0.1.running_mean', 'features.2.0.block.0.1.running_var', 'features.2.0.block.1.0.weight', 'features.2.0.block.1.1.weight', 'features.2.0.block.1.1.bias', 'features.2.0.block.1.1.running_mean', 'features.2.0.block.1.1.running_var', 'features.2.0.block.2.fc1.weight', 'features.2.0.block.2.fc1.bias', 'features.2.0.block.2.fc2.weight', 'features.2.0.block.2.fc2.bias', 'features.2.0.block.3.0.weight', 'features.2.0.block.3.1.weight', 'features.2.0.block.3.1.bias', 'features.2.0.block.3.1.running_mean', 'features.2.0.block.3.1.running_var', 'features.2.1.block.0.0.weight', 'features.2.1.block.0.1.weight', 'features.2.1.block.0.1.bias', 'features.2.1.block.0.1.running_mean', 'features.2.1.block.0.1.running_var', 'features.2.1.block.1.0.weight', 'features.2.1.block.1.1.weight', 'features.2.1.block.1.1.bias', 'features.2.1.block.1.1.running_mean', 'features.2.1.block.1.1.running_var', 'features.2.1.block.2.fc1.weight', 'features.2.1.block.2.fc1.bias', 'features.2.1.block.2.fc2.weight', 'features.2.1.block.2.fc2.bias', 'features.2.1.block.3.0.weight', 'features.2.1.block.3.1.weight', 'features.2.1.block.3.1.bias', 'features.2.1.block.3.1.running_mean', 'features.2.1.block.3.1.running_var', 'features.3.0.block.0.0.weight', 'features.3.0.block.0.1.weight', 'features.3.0.block.0.1.bias', 'features.3.0.block.0.1.running_mean', 'features.3.0.block.0.1.running_var', 'features.3.0.block.1.0.weight', 'features.3.0.block.1.1.weight', 'features.3.0.block.1.1.bias', 'features.3.0.block.1.1.running_mean', 'features.3.0.block.1.1.running_var', 'features.3.0.block.2.fc1.weight', 'features.3.0.block.2.fc1.bias', 'features.3.0.block.2.fc2.weight', 'features.3.0.block.2.fc2.bias', 'features.3.0.block.3.0.weight', 'features.3.0.block.3.1.weight', 'features.3.0.block.3.1.bias', 'features.3.0.block.3.1.running_mean', 'features.3.0.block.3.1.running_var', 'features.3.1.block.0.0.weight', 'features.3.1.block.0.1.weight', 'features.3.1.block.0.1.bias', 'features.3.1.block.0.1.running_mean', 'features.3.1.block.0.1.running_var', 'features.3.1.block.1.0.weight', 'features.3.1.block.1.1.weight', 'features.3.1.block.1.1.bias', 'features.3.1.block.1.1.running_mean', 'features.3.1.block.1.1.running_var', 'features.3.1.block.2.fc1.weight', 'features.3.1.block.2.fc1.bias', 'features.3.1.block.2.fc2.weight', 'features.3.1.block.2.fc2.bias', 'features.3.1.block.3.0.weight', 'features.3.1.block.3.1.weight', 'features.3.1.block.3.1.bias', 'features.3.1.block.3.1.running_mean', 'features.3.1.block.3.1.running_var', 'features.4.0.block.0.0.weight', 'features.4.0.block.0.1.weight', 'features.4.0.block.0.1.bias', 'features.4.0.block.0.1.running_mean', 'features.4.0.block.0.1.running_var', 'features.4.0.block.1.0.weight', 'features.4.0.block.1.1.weight', 'features.4.0.block.1.1.bias', 'features.4.0.block.1.1.running_mean', 'features.4.0.block.1.1.running_var', 'features.4.0.block.2.fc1.weight', 'features.4.0.block.2.fc1.bias', 'features.4.0.block.2.fc2.weight', 'features.4.0.block.2.fc2.bias', 'features.4.0.block.3.0.weight', 'features.4.0.block.3.1.weight', 'features.4.0.block.3.1.bias', 'features.4.0.block.3.1.running_mean', 'features.4.0.block.3.1.running_var', 'features.4.1.block.0.0.weight', 'features.4.1.block.0.1.weight', 'features.4.1.block.0.1.bias', 'features.4.1.block.0.1.running_mean', 'features.4.1.block.0.1.running_var', 'features.4.1.block.1.0.weight', 'features.4.1.block.1.1.weight', 'features.4.1.block.1.1.bias', 'features.4.1.block.1.1.running_mean', 'features.4.1.block.1.1.running_var', 'features.4.1.block.2.fc1.weight', 'features.4.1.block.2.fc1.bias', 'features.4.1.block.2.fc2.weight', 'features.4.1.block.2.fc2.bias', 'features.4.1.block.3.0.weight', 'features.4.1.block.3.1.weight', 'features.4.1.block.3.1.bias', 'features.4.1.block.3.1.running_mean', 'features.4.1.block.3.1.running_var', 'features.4.2.block.0.0.weight', 'features.4.2.block.0.1.weight', 'features.4.2.block.0.1.bias', 'features.4.2.block.0.1.running_mean', 'features.4.2.block.0.1.running_var', 'features.4.2.block.1.0.weight', 'features.4.2.block.1.1.weight', 'features.4.2.block.1.1.bias', 'features.4.2.block.1.1.running_mean', 'features.4.2.block.1.1.running_var', 'features.4.2.block.2.fc1.weight', 'features.4.2.block.2.fc1.bias', 'features.4.2.block.2.fc2.weight', 'features.4.2.block.2.fc2.bias', 'features.4.2.block.3.0.weight', 'features.4.2.block.3.1.weight', 'features.4.2.block.3.1.bias', 'features.4.2.block.3.1.running_mean', 'features.4.2.block.3.1.running_var', 'features.5.0.block.0.0.weight', 'features.5.0.block.0.1.weight', 'features.5.0.block.0.1.bias', 'features.5.0.block.0.1.running_mean', 'features.5.0.block.0.1.running_var', 'features.5.0.block.1.0.weight', 'features.5.0.block.1.1.weight', 'features.5.0.block.1.1.bias', 'features.5.0.block.1.1.running_mean', 'features.5.0.block.1.1.running_var', 'features.5.0.block.2.fc1.weight', 'features.5.0.block.2.fc1.bias', 'features.5.0.block.2.fc2.weight', 'features.5.0.block.2.fc2.bias', 'features.5.0.block.3.0.weight', 'features.5.0.block.3.1.weight', 'features.5.0.block.3.1.bias', 'features.5.0.block.3.1.running_mean', 'features.5.0.block.3.1.running_var', 'features.5.1.block.0.0.weight', 'features.5.1.block.0.1.weight', 'features.5.1.block.0.1.bias', 'features.5.1.block.0.1.running_mean', 'features.5.1.block.0.1.running_var', 'features.5.1.block.1.0.weight', 'features.5.1.block.1.1.weight', 'features.5.1.block.1.1.bias', 'features.5.1.block.1.1.running_mean', 'features.5.1.block.1.1.running_var', 'features.5.1.block.2.fc1.weight', 'features.5.1.block.2.fc1.bias', 'features.5.1.block.2.fc2.weight', 'features.5.1.block.2.fc2.bias', 'features.5.1.block.3.0.weight', 'features.5.1.block.3.1.weight', 'features.5.1.block.3.1.bias', 'features.5.1.block.3.1.running_mean', 'features.5.1.block.3.1.running_var', 'features.5.2.block.0.0.weight', 'features.5.2.block.0.1.weight', 'features.5.2.block.0.1.bias', 'features.5.2.block.0.1.running_mean', 'features.5.2.block.0.1.running_var', 'features.5.2.block.1.0.weight', 'features.5.2.block.1.1.weight', 'features.5.2.block.1.1.bias', 'features.5.2.block.1.1.running_mean', 'features.5.2.block.1.1.running_var', 'features.5.2.block.2.fc1.weight', 'features.5.2.block.2.fc1.bias', 'features.5.2.block.2.fc2.weight', 'features.5.2.block.2.fc2.bias', 'features.5.2.block.3.0.weight', 'features.5.2.block.3.1.weight', 'features.5.2.block.3.1.bias', 'features.5.2.block.3.1.running_mean', 'features.5.2.block.3.1.running_var', 'features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.0.1.running_mean', 'features.6.0.block.0.1.running_var', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.1.1.running_mean', 'features.6.0.block.1.1.running_var', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.0.block.3.1.running_mean', 'features.6.0.block.3.1.running_var', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.0.1.running_mean', 'features.6.1.block.0.1.running_var', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.1.1.running_mean', 'features.6.1.block.1.1.running_var', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.1.block.3.1.running_mean', 'features.6.1.block.3.1.running_var', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.0.1.running_mean', 'features.6.2.block.0.1.running_var', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.1.1.running_mean', 'features.6.2.block.1.1.running_var', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.2.block.3.1.running_mean', 'features.6.2.block.3.1.running_var', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.0.1.running_mean', 'features.6.3.block.0.1.running_var', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.1.1.running_mean', 'features.6.3.block.1.1.running_var', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.3.block.3.1.running_mean', 'features.6.3.block.3.1.running_var', 'features.7.0.block.0.0.weight', 'features.7.0.block.0.1.weight', 'features.7.0.block.0.1.bias', 'features.7.0.block.0.1.running_mean', 'features.7.0.block.0.1.running_var', 'features.7.0.block.1.0.weight', 'features.7.0.block.1.1.weight', 'features.7.0.block.1.1.bias', 'features.7.0.block.1.1.running_mean', 'features.7.0.block.1.1.running_var', 'features.7.0.block.2.fc1.weight', 'features.7.0.block.2.fc1.bias', 'features.7.0.block.2.fc2.weight', 'features.7.0.block.2.fc2.bias', 'features.7.0.block.3.0.weight', 'features.7.0.block.3.1.weight', 'features.7.0.block.3.1.bias', 'features.7.0.block.3.1.running_mean', 'features.7.0.block.3.1.running_var', 'features.8.0.weight', 'features.8.1.weight', 'features.8.1.bias', 'features.8.1.running_mean', 'features.8.1.running_var']\n",
      "Unexpected keys: ['encoder.features.0.0.weight', 'encoder.features.0.1.weight', 'encoder.features.0.1.bias', 'encoder.features.0.1.running_mean', 'encoder.features.0.1.running_var', 'encoder.features.0.1.num_batches_tracked', 'encoder.features.1.0.block.0.0.weight', 'encoder.features.1.0.block.0.1.weight', 'encoder.features.1.0.block.0.1.bias', 'encoder.features.1.0.block.0.1.running_mean', 'encoder.features.1.0.block.0.1.running_var', 'encoder.features.1.0.block.0.1.num_batches_tracked', 'encoder.features.1.0.block.1.fc1.weight', 'encoder.features.1.0.block.1.fc1.bias', 'encoder.features.1.0.block.1.fc2.weight', 'encoder.features.1.0.block.1.fc2.bias', 'encoder.features.1.0.block.2.0.weight', 'encoder.features.1.0.block.2.1.weight', 'encoder.features.1.0.block.2.1.bias', 'encoder.features.1.0.block.2.1.running_mean', 'encoder.features.1.0.block.2.1.running_var', 'encoder.features.1.0.block.2.1.num_batches_tracked', 'encoder.features.2.0.block.0.0.weight', 'encoder.features.2.0.block.0.1.weight', 'encoder.features.2.0.block.0.1.bias', 'encoder.features.2.0.block.0.1.running_mean', 'encoder.features.2.0.block.0.1.running_var', 'encoder.features.2.0.block.0.1.num_batches_tracked', 'encoder.features.2.0.block.1.0.weight', 'encoder.features.2.0.block.1.1.weight', 'encoder.features.2.0.block.1.1.bias', 'encoder.features.2.0.block.1.1.running_mean', 'encoder.features.2.0.block.1.1.running_var', 'encoder.features.2.0.block.1.1.num_batches_tracked', 'encoder.features.2.0.block.2.fc1.weight', 'encoder.features.2.0.block.2.fc1.bias', 'encoder.features.2.0.block.2.fc2.weight', 'encoder.features.2.0.block.2.fc2.bias', 'encoder.features.2.0.block.3.0.weight', 'encoder.features.2.0.block.3.1.weight', 'encoder.features.2.0.block.3.1.bias', 'encoder.features.2.0.block.3.1.running_mean', 'encoder.features.2.0.block.3.1.running_var', 'encoder.features.2.0.block.3.1.num_batches_tracked', 'encoder.features.2.1.block.0.0.weight', 'encoder.features.2.1.block.0.1.weight', 'encoder.features.2.1.block.0.1.bias', 'encoder.features.2.1.block.0.1.running_mean', 'encoder.features.2.1.block.0.1.running_var', 'encoder.features.2.1.block.0.1.num_batches_tracked', 'encoder.features.2.1.block.1.0.weight', 'encoder.features.2.1.block.1.1.weight', 'encoder.features.2.1.block.1.1.bias', 'encoder.features.2.1.block.1.1.running_mean', 'encoder.features.2.1.block.1.1.running_var', 'encoder.features.2.1.block.1.1.num_batches_tracked', 'encoder.features.2.1.block.2.fc1.weight', 'encoder.features.2.1.block.2.fc1.bias', 'encoder.features.2.1.block.2.fc2.weight', 'encoder.features.2.1.block.2.fc2.bias', 'encoder.features.2.1.block.3.0.weight', 'encoder.features.2.1.block.3.1.weight', 'encoder.features.2.1.block.3.1.bias', 'encoder.features.2.1.block.3.1.running_mean', 'encoder.features.2.1.block.3.1.running_var', 'encoder.features.2.1.block.3.1.num_batches_tracked', 'encoder.features.3.0.block.0.0.weight', 'encoder.features.3.0.block.0.1.weight', 'encoder.features.3.0.block.0.1.bias', 'encoder.features.3.0.block.0.1.running_mean', 'encoder.features.3.0.block.0.1.running_var', 'encoder.features.3.0.block.0.1.num_batches_tracked', 'encoder.features.3.0.block.1.0.weight', 'encoder.features.3.0.block.1.1.weight', 'encoder.features.3.0.block.1.1.bias', 'encoder.features.3.0.block.1.1.running_mean', 'encoder.features.3.0.block.1.1.running_var', 'encoder.features.3.0.block.1.1.num_batches_tracked', 'encoder.features.3.0.block.2.fc1.weight', 'encoder.features.3.0.block.2.fc1.bias', 'encoder.features.3.0.block.2.fc2.weight', 'encoder.features.3.0.block.2.fc2.bias', 'encoder.features.3.0.block.3.0.weight', 'encoder.features.3.0.block.3.1.weight', 'encoder.features.3.0.block.3.1.bias', 'encoder.features.3.0.block.3.1.running_mean', 'encoder.features.3.0.block.3.1.running_var', 'encoder.features.3.0.block.3.1.num_batches_tracked', 'encoder.features.3.1.block.0.0.weight', 'encoder.features.3.1.block.0.1.weight', 'encoder.features.3.1.block.0.1.bias', 'encoder.features.3.1.block.0.1.running_mean', 'encoder.features.3.1.block.0.1.running_var', 'encoder.features.3.1.block.0.1.num_batches_tracked', 'encoder.features.3.1.block.1.0.weight', 'encoder.features.3.1.block.1.1.weight', 'encoder.features.3.1.block.1.1.bias', 'encoder.features.3.1.block.1.1.running_mean', 'encoder.features.3.1.block.1.1.running_var', 'encoder.features.3.1.block.1.1.num_batches_tracked', 'encoder.features.3.1.block.2.fc1.weight', 'encoder.features.3.1.block.2.fc1.bias', 'encoder.features.3.1.block.2.fc2.weight', 'encoder.features.3.1.block.2.fc2.bias', 'encoder.features.3.1.block.3.0.weight', 'encoder.features.3.1.block.3.1.weight', 'encoder.features.3.1.block.3.1.bias', 'encoder.features.3.1.block.3.1.running_mean', 'encoder.features.3.1.block.3.1.running_var', 'encoder.features.3.1.block.3.1.num_batches_tracked', 'encoder.features.4.0.block.0.0.weight', 'encoder.features.4.0.block.0.1.weight', 'encoder.features.4.0.block.0.1.bias', 'encoder.features.4.0.block.0.1.running_mean', 'encoder.features.4.0.block.0.1.running_var', 'encoder.features.4.0.block.0.1.num_batches_tracked', 'encoder.features.4.0.block.1.0.weight', 'encoder.features.4.0.block.1.1.weight', 'encoder.features.4.0.block.1.1.bias', 'encoder.features.4.0.block.1.1.running_mean', 'encoder.features.4.0.block.1.1.running_var', 'encoder.features.4.0.block.1.1.num_batches_tracked', 'encoder.features.4.0.block.2.fc1.weight', 'encoder.features.4.0.block.2.fc1.bias', 'encoder.features.4.0.block.2.fc2.weight', 'encoder.features.4.0.block.2.fc2.bias', 'encoder.features.4.0.block.3.0.weight', 'encoder.features.4.0.block.3.1.weight', 'encoder.features.4.0.block.3.1.bias', 'encoder.features.4.0.block.3.1.running_mean', 'encoder.features.4.0.block.3.1.running_var', 'encoder.features.4.0.block.3.1.num_batches_tracked', 'encoder.features.4.1.block.0.0.weight', 'encoder.features.4.1.block.0.1.weight', 'encoder.features.4.1.block.0.1.bias', 'encoder.features.4.1.block.0.1.running_mean', 'encoder.features.4.1.block.0.1.running_var', 'encoder.features.4.1.block.0.1.num_batches_tracked', 'encoder.features.4.1.block.1.0.weight', 'encoder.features.4.1.block.1.1.weight', 'encoder.features.4.1.block.1.1.bias', 'encoder.features.4.1.block.1.1.running_mean', 'encoder.features.4.1.block.1.1.running_var', 'encoder.features.4.1.block.1.1.num_batches_tracked', 'encoder.features.4.1.block.2.fc1.weight', 'encoder.features.4.1.block.2.fc1.bias', 'encoder.features.4.1.block.2.fc2.weight', 'encoder.features.4.1.block.2.fc2.bias', 'encoder.features.4.1.block.3.0.weight', 'encoder.features.4.1.block.3.1.weight', 'encoder.features.4.1.block.3.1.bias', 'encoder.features.4.1.block.3.1.running_mean', 'encoder.features.4.1.block.3.1.running_var', 'encoder.features.4.1.block.3.1.num_batches_tracked', 'encoder.features.4.2.block.0.0.weight', 'encoder.features.4.2.block.0.1.weight', 'encoder.features.4.2.block.0.1.bias', 'encoder.features.4.2.block.0.1.running_mean', 'encoder.features.4.2.block.0.1.running_var', 'encoder.features.4.2.block.0.1.num_batches_tracked', 'encoder.features.4.2.block.1.0.weight', 'encoder.features.4.2.block.1.1.weight', 'encoder.features.4.2.block.1.1.bias', 'encoder.features.4.2.block.1.1.running_mean', 'encoder.features.4.2.block.1.1.running_var', 'encoder.features.4.2.block.1.1.num_batches_tracked', 'encoder.features.4.2.block.2.fc1.weight', 'encoder.features.4.2.block.2.fc1.bias', 'encoder.features.4.2.block.2.fc2.weight', 'encoder.features.4.2.block.2.fc2.bias', 'encoder.features.4.2.block.3.0.weight', 'encoder.features.4.2.block.3.1.weight', 'encoder.features.4.2.block.3.1.bias', 'encoder.features.4.2.block.3.1.running_mean', 'encoder.features.4.2.block.3.1.running_var', 'encoder.features.4.2.block.3.1.num_batches_tracked', 'encoder.features.5.0.block.0.0.weight', 'encoder.features.5.0.block.0.1.weight', 'encoder.features.5.0.block.0.1.bias', 'encoder.features.5.0.block.0.1.running_mean', 'encoder.features.5.0.block.0.1.running_var', 'encoder.features.5.0.block.0.1.num_batches_tracked', 'encoder.features.5.0.block.1.0.weight', 'encoder.features.5.0.block.1.1.weight', 'encoder.features.5.0.block.1.1.bias', 'encoder.features.5.0.block.1.1.running_mean', 'encoder.features.5.0.block.1.1.running_var', 'encoder.features.5.0.block.1.1.num_batches_tracked', 'encoder.features.5.0.block.2.fc1.weight', 'encoder.features.5.0.block.2.fc1.bias', 'encoder.features.5.0.block.2.fc2.weight', 'encoder.features.5.0.block.2.fc2.bias', 'encoder.features.5.0.block.3.0.weight', 'encoder.features.5.0.block.3.1.weight', 'encoder.features.5.0.block.3.1.bias', 'encoder.features.5.0.block.3.1.running_mean', 'encoder.features.5.0.block.3.1.running_var', 'encoder.features.5.0.block.3.1.num_batches_tracked', 'encoder.features.5.1.block.0.0.weight', 'encoder.features.5.1.block.0.1.weight', 'encoder.features.5.1.block.0.1.bias', 'encoder.features.5.1.block.0.1.running_mean', 'encoder.features.5.1.block.0.1.running_var', 'encoder.features.5.1.block.0.1.num_batches_tracked', 'encoder.features.5.1.block.1.0.weight', 'encoder.features.5.1.block.1.1.weight', 'encoder.features.5.1.block.1.1.bias', 'encoder.features.5.1.block.1.1.running_mean', 'encoder.features.5.1.block.1.1.running_var', 'encoder.features.5.1.block.1.1.num_batches_tracked', 'encoder.features.5.1.block.2.fc1.weight', 'encoder.features.5.1.block.2.fc1.bias', 'encoder.features.5.1.block.2.fc2.weight', 'encoder.features.5.1.block.2.fc2.bias', 'encoder.features.5.1.block.3.0.weight', 'encoder.features.5.1.block.3.1.weight', 'encoder.features.5.1.block.3.1.bias', 'encoder.features.5.1.block.3.1.running_mean', 'encoder.features.5.1.block.3.1.running_var', 'encoder.features.5.1.block.3.1.num_batches_tracked', 'encoder.features.5.2.block.0.0.weight', 'encoder.features.5.2.block.0.1.weight', 'encoder.features.5.2.block.0.1.bias', 'encoder.features.5.2.block.0.1.running_mean', 'encoder.features.5.2.block.0.1.running_var', 'encoder.features.5.2.block.0.1.num_batches_tracked', 'encoder.features.5.2.block.1.0.weight', 'encoder.features.5.2.block.1.1.weight', 'encoder.features.5.2.block.1.1.bias', 'encoder.features.5.2.block.1.1.running_mean', 'encoder.features.5.2.block.1.1.running_var', 'encoder.features.5.2.block.1.1.num_batches_tracked', 'encoder.features.5.2.block.2.fc1.weight', 'encoder.features.5.2.block.2.fc1.bias', 'encoder.features.5.2.block.2.fc2.weight', 'encoder.features.5.2.block.2.fc2.bias', 'encoder.features.5.2.block.3.0.weight', 'encoder.features.5.2.block.3.1.weight', 'encoder.features.5.2.block.3.1.bias', 'encoder.features.5.2.block.3.1.running_mean', 'encoder.features.5.2.block.3.1.running_var', 'encoder.features.5.2.block.3.1.num_batches_tracked', 'encoder.features.6.0.block.0.0.weight', 'encoder.features.6.0.block.0.1.weight', 'encoder.features.6.0.block.0.1.bias', 'encoder.features.6.0.block.0.1.running_mean', 'encoder.features.6.0.block.0.1.running_var', 'encoder.features.6.0.block.0.1.num_batches_tracked', 'encoder.features.6.0.block.1.0.weight', 'encoder.features.6.0.block.1.1.weight', 'encoder.features.6.0.block.1.1.bias', 'encoder.features.6.0.block.1.1.running_mean', 'encoder.features.6.0.block.1.1.running_var', 'encoder.features.6.0.block.1.1.num_batches_tracked', 'encoder.features.6.0.block.2.fc1.weight', 'encoder.features.6.0.block.2.fc1.bias', 'encoder.features.6.0.block.2.fc2.weight', 'encoder.features.6.0.block.2.fc2.bias', 'encoder.features.6.0.block.3.0.weight', 'encoder.features.6.0.block.3.1.weight', 'encoder.features.6.0.block.3.1.bias', 'encoder.features.6.0.block.3.1.running_mean', 'encoder.features.6.0.block.3.1.running_var', 'encoder.features.6.0.block.3.1.num_batches_tracked', 'encoder.features.6.1.block.0.0.weight', 'encoder.features.6.1.block.0.1.weight', 'encoder.features.6.1.block.0.1.bias', 'encoder.features.6.1.block.0.1.running_mean', 'encoder.features.6.1.block.0.1.running_var', 'encoder.features.6.1.block.0.1.num_batches_tracked', 'encoder.features.6.1.block.1.0.weight', 'encoder.features.6.1.block.1.1.weight', 'encoder.features.6.1.block.1.1.bias', 'encoder.features.6.1.block.1.1.running_mean', 'encoder.features.6.1.block.1.1.running_var', 'encoder.features.6.1.block.1.1.num_batches_tracked', 'encoder.features.6.1.block.2.fc1.weight', 'encoder.features.6.1.block.2.fc1.bias', 'encoder.features.6.1.block.2.fc2.weight', 'encoder.features.6.1.block.2.fc2.bias', 'encoder.features.6.1.block.3.0.weight', 'encoder.features.6.1.block.3.1.weight', 'encoder.features.6.1.block.3.1.bias', 'encoder.features.6.1.block.3.1.running_mean', 'encoder.features.6.1.block.3.1.running_var', 'encoder.features.6.1.block.3.1.num_batches_tracked', 'encoder.features.6.2.block.0.0.weight', 'encoder.features.6.2.block.0.1.weight', 'encoder.features.6.2.block.0.1.bias', 'encoder.features.6.2.block.0.1.running_mean', 'encoder.features.6.2.block.0.1.running_var', 'encoder.features.6.2.block.0.1.num_batches_tracked', 'encoder.features.6.2.block.1.0.weight', 'encoder.features.6.2.block.1.1.weight', 'encoder.features.6.2.block.1.1.bias', 'encoder.features.6.2.block.1.1.running_mean', 'encoder.features.6.2.block.1.1.running_var', 'encoder.features.6.2.block.1.1.num_batches_tracked', 'encoder.features.6.2.block.2.fc1.weight', 'encoder.features.6.2.block.2.fc1.bias', 'encoder.features.6.2.block.2.fc2.weight', 'encoder.features.6.2.block.2.fc2.bias', 'encoder.features.6.2.block.3.0.weight', 'encoder.features.6.2.block.3.1.weight', 'encoder.features.6.2.block.3.1.bias', 'encoder.features.6.2.block.3.1.running_mean', 'encoder.features.6.2.block.3.1.running_var', 'encoder.features.6.2.block.3.1.num_batches_tracked', 'encoder.features.6.3.block.0.0.weight', 'encoder.features.6.3.block.0.1.weight', 'encoder.features.6.3.block.0.1.bias', 'encoder.features.6.3.block.0.1.running_mean', 'encoder.features.6.3.block.0.1.running_var', 'encoder.features.6.3.block.0.1.num_batches_tracked', 'encoder.features.6.3.block.1.0.weight', 'encoder.features.6.3.block.1.1.weight', 'encoder.features.6.3.block.1.1.bias', 'encoder.features.6.3.block.1.1.running_mean', 'encoder.features.6.3.block.1.1.running_var', 'encoder.features.6.3.block.1.1.num_batches_tracked', 'encoder.features.6.3.block.2.fc1.weight', 'encoder.features.6.3.block.2.fc1.bias', 'encoder.features.6.3.block.2.fc2.weight', 'encoder.features.6.3.block.2.fc2.bias', 'encoder.features.6.3.block.3.0.weight', 'encoder.features.6.3.block.3.1.weight', 'encoder.features.6.3.block.3.1.bias', 'encoder.features.6.3.block.3.1.running_mean', 'encoder.features.6.3.block.3.1.running_var', 'encoder.features.6.3.block.3.1.num_batches_tracked', 'encoder.features.7.0.block.0.0.weight', 'encoder.features.7.0.block.0.1.weight', 'encoder.features.7.0.block.0.1.bias', 'encoder.features.7.0.block.0.1.running_mean', 'encoder.features.7.0.block.0.1.running_var', 'encoder.features.7.0.block.0.1.num_batches_tracked', 'encoder.features.7.0.block.1.0.weight', 'encoder.features.7.0.block.1.1.weight', 'encoder.features.7.0.block.1.1.bias', 'encoder.features.7.0.block.1.1.running_mean', 'encoder.features.7.0.block.1.1.running_var', 'encoder.features.7.0.block.1.1.num_batches_tracked', 'encoder.features.7.0.block.2.fc1.weight', 'encoder.features.7.0.block.2.fc1.bias', 'encoder.features.7.0.block.2.fc2.weight', 'encoder.features.7.0.block.2.fc2.bias', 'encoder.features.7.0.block.3.0.weight', 'encoder.features.7.0.block.3.1.weight', 'encoder.features.7.0.block.3.1.bias', 'encoder.features.7.0.block.3.1.running_mean', 'encoder.features.7.0.block.3.1.running_var', 'encoder.features.7.0.block.3.1.num_batches_tracked', 'encoder.features.8.0.weight', 'encoder.features.8.1.weight', 'encoder.features.8.1.bias', 'encoder.features.8.1.running_mean', 'encoder.features.8.1.running_var', 'encoder.features.8.1.num_batches_tracked', 'projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█| 670/670 [06:26<00:00,  1.73it/s, loss=2.2602, main=1.7507, mouth=1.6984, lr=1.2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 | loss=2.2742 | main=1.7446 | mouth=1.7653\n",
      "[Val]   Epoch 1 | loss=3.0905 | acc=19.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|█| 670/670 [05:59<00:00,  1.86it/s, loss=2.5395, main=1.9797, mouth=1.8660, lr=2.4e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 | loss=2.1875 | main=1.6821 | mouth=1.6848\n",
      "[Val]   Epoch 2 | loss=2.6559 | acc=13.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|█| 670/670 [05:36<00:00,  1.99it/s, loss=2.3538, main=1.7717, mouth=1.9402, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3 | loss=2.1675 | main=1.6622 | mouth=1.6844\n",
      "[Val]   Epoch 3 | loss=2.4776 | acc=17.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|█| 670/670 [05:35<00:00,  2.00it/s, loss=1.5274, main=1.1319, mouth=1.3185, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4 | loss=2.0562 | main=1.5650 | mouth=1.6373\n",
      "[Val]   Epoch 4 | loss=2.2726 | acc=20.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|█| 670/670 [05:33<00:00,  2.01it/s, loss=2.5429, main=1.9021, mouth=2.1360, lr=2.9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5 | loss=1.8633 | main=1.3997 | mouth=1.5455\n",
      "[Val]   Epoch 5 | loss=2.2474 | acc=23.33%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|█| 670/670 [05:30<00:00,  2.03it/s, loss=1.6601, main=1.1925, mouth=1.5586, lr=2.8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6 | loss=1.7254 | main=1.2846 | mouth=1.4695\n",
      "[Val]   Epoch 6 | loss=2.2257 | acc=27.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|█| 670/670 [12:10<00:00,  1.09s/it, loss=1.5363, main=1.0632, mouth=1.5773, lr=2.7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7 | loss=1.6358 | main=1.2076 | mouth=1.4277\n",
      "[Val]   Epoch 7 | loss=2.1448 | acc=31.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|█| 670/670 [05:47<00:00,  1.93it/s, loss=1.6059, main=1.2091, mouth=1.3224, lr=2.6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8 | loss=1.5587 | main=1.1444 | mouth=1.3809\n",
      "[Val]   Epoch 8 | loss=2.0952 | acc=34.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|█| 670/670 [11:14<00:00,  1.01s/it, loss=1.3554, main=0.9519, mouth=1.3450, lr=2.5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9 | loss=1.5074 | main=1.1020 | mouth=1.3512\n",
      "[Val]   Epoch 9 | loss=1.9694 | acc=37.52%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|█| 670/670 [05:23<00:00,  2.07it/s, loss=1.6476, main=1.2935, mouth=1.1804, lr=2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10 | loss=1.4315 | main=1.0386 | mouth=1.3098\n",
      "[Val]   Epoch 10 | loss=2.0920 | acc=37.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|█| 670/670 [05:25<00:00,  2.06it/s, loss=1.8090, main=1.2546, mouth=1.8480, lr=2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11 | loss=1.4014 | main=1.0111 | mouth=1.3007\n",
      "[Val]   Epoch 11 | loss=2.0557 | acc=38.89%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|█| 670/670 [05:22<00:00,  2.08it/s, loss=1.9196, main=1.2796, mouth=2.1333, lr=2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12 | loss=1.3523 | main=0.9693 | mouth=1.2767\n",
      "[Val]   Epoch 12 | loss=1.9069 | acc=41.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|█| 670/670 [07:01<00:00,  1.59it/s, loss=1.3648, main=0.9538, mouth=1.3702, lr=1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13 | loss=1.2967 | main=0.9240 | mouth=1.2422\n",
      "[Val]   Epoch 13 | loss=1.8844 | acc=43.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|█| 670/670 [15:03<00:00,  1.35s/it, loss=1.3152, main=0.9791, mouth=1.1205, lr=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14 | loss=1.2485 | main=0.8850 | mouth=1.2115\n",
      "[Val]   Epoch 14 | loss=1.7918 | acc=45.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|█| 670/670 [21:55<00:00,  1.96s/it, loss=1.6675, main=1.2600, mouth=1.3585, lr=1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15 | loss=1.2130 | main=0.8550 | mouth=1.1934\n",
      "[Val]   Epoch 15 | loss=1.8438 | acc=45.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|█| 670/670 [05:26<00:00,  2.05it/s, loss=1.2647, main=0.7964, mouth=1.5611, lr=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16 | loss=1.1731 | main=0.8227 | mouth=1.1682\n",
      "[Val]   Epoch 16 | loss=1.7858 | acc=49.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=0.8631, main=0.5221, mouth=1.1368, lr=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17 | loss=1.1319 | main=0.7869 | mouth=1.1500\n",
      "[Val]   Epoch 17 | loss=1.7948 | acc=48.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=0.8215, main=0.5214, mouth=1.0002, lr=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18 | loss=1.1077 | main=0.7641 | mouth=1.1456\n",
      "[Val]   Epoch 18 | loss=1.8106 | acc=49.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.0230, main=0.6350, mouth=1.2935, lr=7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19 | loss=1.0664 | main=0.7322 | mouth=1.1140\n",
      "[Val]   Epoch 19 | loss=1.8072 | acc=50.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.4669, main=0.9529, mouth=1.7132, lr=6.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20 | loss=1.0511 | main=0.7210 | mouth=1.1002\n",
      "[Val]   Epoch 20 | loss=1.7378 | acc=52.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.3527, main=0.9266, mouth=1.4204, lr=5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21 | loss=1.0367 | main=0.7102 | mouth=1.0881\n",
      "[Val]   Epoch 21 | loss=1.7187 | acc=53.08%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=0.8636, main=0.5541, mouth=1.0316, lr=4.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22 | loss=1.0034 | main=0.6824 | mouth=1.0699\n",
      "[Val]   Epoch 22 | loss=1.7528 | acc=52.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|█| 670/670 [05:20<00:00,  2.09it/s, loss=1.2417, main=0.7842, mouth=1.5249, lr=3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23 | loss=0.9960 | main=0.6727 | mouth=1.0778\n",
      "[Val]   Epoch 23 | loss=1.7082 | acc=53.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=1.0739, main=0.6766, mouth=1.3243, lr=3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24 | loss=0.9841 | main=0.6646 | mouth=1.0650\n",
      "[Val]   Epoch 24 | loss=1.7482 | acc=52.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|█| 670/670 [05:19<00:00,  2.10it/s, loss=0.6576, main=0.4376, mouth=0.7334, lr=3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25 | loss=0.9772 | main=0.6605 | mouth=1.0557\n",
      "[Val]   Epoch 25 | loss=1.7296 | acc=53.60%\n",
      "\n",
      "Saved model to affectnet_7cls_mouth_v4_angry_boost.pth\n"
     ]
    }
   ],
   "source": [
    "# fine_tune_7cls_mouth_v4.py\n",
    "# 7 classes + SimCLR backbone + class weight（Angry/Disgust 加权）+ balanced sampler + mouth auxiliary head (de-emphasise)\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT   = \"YOLO_format_cls\"\n",
    "TRAIN_DIR   = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR     = os.path.join(DATA_ROOT, \"valid\")\n",
    "\n",
    "SIMCLR_PATH = \"tinysimclr_effb0_mac.pth\"\n",
    "SAVE_PATH   = \"affectnet_7cls_mouth_v4_angry_boost.pth\"\n",
    "\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 25\n",
    "BASE_LR     = 3e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1. collate_fn: Preserve PIL images\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)   # imgs: list[PIL.Image], labels: list[int]\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "\n",
    "# 2. Data augmentation (real-time compatible)\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mouth_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Simple 'mouth crop'\n",
    "def crop_mouth_pil(img):\n",
    "    w, h = img.size\n",
    "    top = int(h * 0.55)\n",
    "    return img.crop((0, top, w, h))\n",
    "\n",
    "\n",
    "# 4. dataset + Balanced Sampler + class weight（enhance Angry/Disgust）\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=None)\n",
    "val_set   = datasets.ImageFolder(VAL_DIR, transform=None)\n",
    "\n",
    "print(f\"Train size (7-class): {len(train_set)}, Val size: {len(val_set)}\")\n",
    "print(\"Class index mapping:\", train_set.class_to_idx)\n",
    "\n",
    "orig_targets = train_set.targets\n",
    "counter_7 = Counter(orig_targets)\n",
    "print(\"7-class counts in train:\", counter_7)\n",
    "\n",
    "# basic class weight: 1/count\n",
    "base_class_weights = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    count_c = counter_7.get(c, 1)\n",
    "    base_class_weights.append(1.0 / count_c)\n",
    "\n",
    "# find the index of angry / disgusted \n",
    "angry_idx     = train_set.class_to_idx.get(\"angry\", 0)\n",
    "disgust_idx   = train_set.class_to_idx.get(\"disgusted\", 1)\n",
    "\n",
    "# Apply amplified weighting specifically to Angry & Disgust (e.g. x3)\n",
    "ANGRY_BOOST   = 3.0\n",
    "DISGUST_BOOST = 2.0\n",
    "\n",
    "base_class_weights[angry_idx]   *= ANGRY_BOOST\n",
    "base_class_weights[disgust_idx] *= DISGUST_BOOST\n",
    "\n",
    "# Normalised to the mean ~1\n",
    "sum_w = sum(base_class_weights)\n",
    "class_weights = [w / sum_w * NUM_CLASSES for w in base_class_weights]\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights (for CE, after angry boost):\", class_weights_tensor.tolist())\n",
    "\n",
    "# Balanced Sampler\n",
    "sample_weights = [class_weights[label] for label in orig_targets]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# 5.Backbone + Mouth Auxiliary Head\n",
    "class FER7WithMouth(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        feat_full  = self.backbone(x_full)\n",
    "        feat_mouth = self.backbone(x_mouth)\n",
    "\n",
    "        logits_main  = self.head_main(feat_full)\n",
    "        logits_mouth = self.head_mouth(feat_mouth)\n",
    "        return logits_main, logits_mouth\n",
    "\n",
    "    def forward_main(self, x_full):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        return self.head_main(feat_full)\n",
    "\n",
    "\n",
    "base = efficientnet_b0(weights=None)\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "print(f\"Loading SimCLR weights from {SIMCLR_PATH} ...\")\n",
    "simclr_weights = torch.load(SIMCLR_PATH, map_location=\"cpu\")\n",
    "missing, unexpected = base.load_state_dict(simclr_weights, strict=False)\n",
    "print(\"Loaded SimCLR backbone.\")\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "model = FER7WithMouth(base, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# CE with class weight + label smoothing\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=class_weights_tensor,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Reduce the weight of the mouth auxiliary head to prevent it from overshadowing the eyebrow features\n",
    "LAMBDA_MOUTH = 0.3\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "\n",
    "# 6. LR warmup + cosine decay\n",
    "total_steps  = EPOCHS * len(train_loader)\n",
    "warmup_ratio = 0.1\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return BASE_LR * float(step + 1) / float(warmup_steps + 1)\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    min_lr = BASE_LR * 0.1\n",
    "    return min_lr + (BASE_LR - min_lr) * cosine\n",
    "\n",
    "# 7. Training cycle\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    ce_main_sum = 0.0\n",
    "    ce_mouth_sum = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=100)\n",
    "    for imgs, labels in pbar:\n",
    "        y = torch.tensor(labels, dtype=torch.long).to(device)  # [B]\n",
    "\n",
    "        x_full = torch.stack([train_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "        mouth_imgs = [crop_mouth_pil(img) for img in imgs]\n",
    "        x_mouth = torch.stack([mouth_aug(m) for m in mouth_imgs]).to(device)\n",
    "\n",
    "        lr = get_lr(global_step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "        logits_main, logits_mouth = model(x_full, x_mouth)\n",
    "\n",
    "        ce_main  = criterion(logits_main, y)\n",
    "        ce_mouth = criterion(logits_mouth, y)\n",
    "\n",
    "        loss = ce_main + LAMBDA_MOUTH * ce_mouth\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss  += loss.item()\n",
    "        ce_main_sum += ce_main.item()\n",
    "        ce_mouth_sum += ce_mouth.item()\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\":  f\"{loss.item():.4f}\",\n",
    "            \"main\":  f\"{ce_main.item():.4f}\",\n",
    "            \"mouth\": f\"{ce_mouth.item():.4f}\",\n",
    "            \"lr\":    f\"{lr:.1e}\"\n",
    "        })\n",
    "\n",
    "    avg_loss  = train_loss / len(train_loader)\n",
    "    avg_main  = ce_main_sum / len(train_loader)\n",
    "    avg_mouth = ce_mouth_sum / len(train_loader)\n",
    "    print(f\"[Train] Epoch {epoch+1} | loss={avg_loss:.4f} | main={avg_main:.4f} | mouth={avg_mouth:.4f}\")\n",
    "\n",
    "    # Verification (Main Header Only)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            y_val = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            x_val = torch.stack([val_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "            logits_val = model.forward_main(x_val)\n",
    "            loss_val = criterion(logits_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = logits_val.argmax(dim=1)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "            total += len(y_val)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc   = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Val]   Epoch {epoch+1} | loss={val_loss:.4f} | acc={val_acc*100:.2f}%\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Saved model to {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train size (7-class): 10714, Val size: 3129\n",
      "Class index mapping: {'angry': 0, 'disgusted': 1, 'fearful': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprised': 6}\n",
      "7-class counts in train: Counter({6: 2248, 0: 2026, 2: 1922, 1: 1865, 5: 1582, 4: 744, 3: 327})\n",
      "Class weights (for CE, after angry/disgust boost): [1.2119460105895996, 0.8777132034301758, 0.4258415997028351, 2.5029587745666504, 1.100090742111206, 0.5173625349998474, 0.36408698558807373]\n",
      "Building MobileNetV2 backbone...\n",
      "Model built with MobileNetV2 backbone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█| 670/670 [03:52<00:00,  2.89it/s, loss=1.8538, main=1.4371, mouth=1.3890, lr=1.2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 | loss=2.2459 | main=1.7258 | mouth=1.7338\n",
      "[Val]   Epoch 1 | loss=2.4114 | acc=19.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|█| 670/670 [03:18<00:00,  3.37it/s, loss=2.1261, main=1.6438, mouth=1.6077, lr=2.4e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2 | loss=2.2138 | main=1.7033 | mouth=1.7016\n",
      "[Val]   Epoch 2 | loss=2.4714 | acc=19.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|█| 670/670 [03:20<00:00,  3.33it/s, loss=2.8448, main=2.1931, mouth=2.1725, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3 | loss=2.1755 | main=1.6728 | mouth=1.6754\n",
      "[Val]   Epoch 3 | loss=2.4518 | acc=19.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|█| 670/670 [03:19<00:00,  3.36it/s, loss=2.1182, main=1.5575, mouth=1.8690, lr=3.0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4 | loss=2.0915 | main=1.6011 | mouth=1.6345\n",
      "[Val]   Epoch 4 | loss=2.5299 | acc=18.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|█| 670/670 [03:19<00:00,  3.36it/s, loss=2.0827, main=1.5773, mouth=1.6847, lr=2.9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5 | loss=1.9352 | main=1.4697 | mouth=1.5514\n",
      "[Val]   Epoch 5 | loss=2.5812 | acc=21.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|█| 670/670 [03:19<00:00,  3.36it/s, loss=2.0630, main=1.5981, mouth=1.5495, lr=2.8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6 | loss=1.8482 | main=1.3944 | mouth=1.5128\n",
      "[Val]   Epoch 6 | loss=2.3072 | acc=23.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|█| 670/670 [03:13<00:00,  3.46it/s, loss=1.6361, main=1.2287, mouth=1.3581, lr=2.7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7 | loss=1.7346 | main=1.2995 | mouth=1.4501\n",
      "[Val]   Epoch 7 | loss=2.3255 | acc=22.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|█| 670/670 [03:14<00:00,  3.44it/s, loss=1.3337, main=1.0493, mouth=0.9480, lr=2.6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8 | loss=1.6642 | main=1.2337 | mouth=1.4349\n",
      "[Val]   Epoch 8 | loss=2.1592 | acc=29.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|█| 670/670 [03:14<00:00,  3.45it/s, loss=1.9590, main=1.4724, mouth=1.6221, lr=2.5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9 | loss=1.6147 | main=1.1947 | mouth=1.3999\n",
      "[Val]   Epoch 9 | loss=2.2266 | acc=30.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|█| 670/670 [03:15<00:00,  3.43it/s, loss=0.8917, main=0.6126, mouth=0.9305, lr=2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10 | loss=1.5419 | main=1.1316 | mouth=1.3675\n",
      "[Val]   Epoch 10 | loss=2.1195 | acc=33.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|█| 670/670 [03:14<00:00,  3.45it/s, loss=1.8939, main=1.5141, mouth=1.2658, lr=2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11 | loss=1.4856 | main=1.0847 | mouth=1.3362\n",
      "[Val]   Epoch 11 | loss=2.0611 | acc=34.61%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|█| 670/670 [03:14<00:00,  3.45it/s, loss=1.7246, main=1.2488, mouth=1.5857, lr=2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12 | loss=1.4344 | main=1.0426 | mouth=1.3058\n",
      "[Val]   Epoch 12 | loss=2.1539 | acc=34.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|█| 670/670 [03:12<00:00,  3.48it/s, loss=1.1004, main=0.7799, mouth=1.0685, lr=1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13 | loss=1.3928 | main=1.0059 | mouth=1.2896\n",
      "[Val]   Epoch 13 | loss=2.0392 | acc=36.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|█| 670/670 [03:12<00:00,  3.48it/s, loss=1.2559, main=0.8797, mouth=1.2539, lr=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14 | loss=1.3578 | main=0.9743 | mouth=1.2783\n",
      "[Val]   Epoch 14 | loss=1.8897 | acc=38.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|█| 670/670 [03:13<00:00,  3.46it/s, loss=1.5726, main=1.1192, mouth=1.5113, lr=1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15 | loss=1.3063 | main=0.9340 | mouth=1.2411\n",
      "[Val]   Epoch 15 | loss=2.0053 | acc=38.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|█| 670/670 [03:15<00:00,  3.43it/s, loss=1.1437, main=0.8625, mouth=0.9373, lr=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16 | loss=1.2882 | main=0.9183 | mouth=1.2330\n",
      "[Val]   Epoch 16 | loss=1.9515 | acc=40.75%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|█| 670/670 [03:15<00:00,  3.43it/s, loss=2.1009, main=1.5995, mouth=1.6713, lr=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17 | loss=1.2503 | main=0.8833 | mouth=1.2230\n",
      "[Val]   Epoch 17 | loss=1.9831 | acc=39.25%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|█| 670/670 [03:16<00:00,  3.40it/s, loss=0.7776, main=0.5358, mouth=0.8061, lr=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18 | loss=1.2085 | main=0.8481 | mouth=1.2014\n",
      "[Val]   Epoch 18 | loss=1.9134 | acc=39.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|█| 670/670 [03:16<00:00,  3.41it/s, loss=0.9949, main=0.6548, mouth=1.1336, lr=7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19 | loss=1.1508 | main=0.8072 | mouth=1.1452\n",
      "[Val]   Epoch 19 | loss=1.9401 | acc=41.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|█| 670/670 [03:17<00:00,  3.39it/s, loss=1.4786, main=0.9644, mouth=1.7143, lr=6.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20 | loss=1.1401 | main=0.7940 | mouth=1.1536\n",
      "[Val]   Epoch 20 | loss=1.8812 | acc=42.89%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|█| 670/670 [03:20<00:00,  3.35it/s, loss=1.2344, main=0.7820, mouth=1.5081, lr=5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21 | loss=1.1203 | main=0.7758 | mouth=1.1483\n",
      "[Val]   Epoch 21 | loss=1.9218 | acc=42.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|█| 670/670 [03:27<00:00,  3.22it/s, loss=0.8680, main=0.6317, mouth=0.7876, lr=4.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22 | loss=1.0945 | main=0.7556 | mouth=1.1296\n",
      "[Val]   Epoch 22 | loss=1.9073 | acc=42.35%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|█| 670/670 [03:19<00:00,  3.36it/s, loss=1.2236, main=0.8284, mouth=1.3174, lr=3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23 | loss=1.0888 | main=0.7515 | mouth=1.1244\n",
      "[Val]   Epoch 23 | loss=1.8694 | acc=43.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|█| 670/670 [03:28<00:00,  3.22it/s, loss=0.9866, main=0.6708, mouth=1.0525, lr=3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24 | loss=1.0717 | main=0.7361 | mouth=1.1186\n",
      "[Val]   Epoch 24 | loss=1.9091 | acc=43.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|█| 670/670 [03:25<00:00,  3.26it/s, loss=1.4298, main=0.9390, mouth=1.6360, lr=3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25 | loss=1.0629 | main=0.7309 | mouth=1.1066\n",
      "[Val]   Epoch 25 | loss=1.8951 | acc=43.98%\n",
      "\n",
      "Saved MobileNetV2-based model to affectnet_7cls_mouth_mobilenet.pth\n"
     ]
    }
   ],
   "source": [
    "# fine_tune_7cls_mouth_mobilenet.py\n",
    "# 7 classes + MobileNetV2 backbone + class weight（Angry/Disgust 加权）\n",
    "# + balanced sampler + mouth auxiliary head (de-emphasised)\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import mobilenet_v2\n",
    "try:\n",
    "    from torchvision.models import MobileNet_V2_Weights\n",
    "    HAS_WEIGHTS_ENUM = True\n",
    "except Exception:\n",
    "    HAS_WEIGHTS_ENUM = False\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_ROOT   = \"YOLO_format_cls\"\n",
    "TRAIN_DIR   = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR     = os.path.join(DATA_ROOT, \"valid\")\n",
    "\n",
    "SAVE_PATH   = \"affectnet_7cls_mouth_mobilenet.pth\"\n",
    "\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 25\n",
    "BASE_LR     = 3e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. collate_fn: Preserve PIL image\n",
    "# =========================\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)   # imgs: list[PIL.Image], labels: list[int]\n",
    "    return list(imgs), list(labels)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Data Augmentation (Real-Time Friendly)\n",
    "# =========================\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mouth_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. 简单的“嘴部 crop”\n",
    "# =========================\n",
    "def crop_mouth_pil(img):\n",
    "    \"\"\"\n",
    "    Directly remove 45% as the mouth area.\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    top = int(h * 0.55)\n",
    "    return img.crop((0, top, w, h))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. dataset + Balanced Sampler + class weight\n",
    "# =========================\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=None)\n",
    "val_set   = datasets.ImageFolder(VAL_DIR, transform=None)\n",
    "\n",
    "print(f\"Train size (7-class): {len(train_set)}, Val size: {len(val_set)}\")\n",
    "print(\"Class index mapping:\", train_set.class_to_idx)\n",
    "\n",
    "orig_targets = train_set.targets\n",
    "counter_7 = Counter(orig_targets)\n",
    "print(\"7-class counts in train:\", counter_7)\n",
    "\n",
    "# ----- class weight: 1/count -----\n",
    "base_class_weights = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    count_c = counter_7.get(c, 1)\n",
    "    base_class_weights.append(1.0 / count_c)\n",
    "\n",
    "# angry / disgusted index\n",
    "angry_idx   = train_set.class_to_idx.get(\"angry\", 0)\n",
    "disgust_idx = train_set.class_to_idx.get(\"disgusted\", 1)\n",
    "\n",
    "# Increase the weight for Angry & Disgust (e.g., ×3, ×2)\n",
    "ANGRY_BOOST   = 3.0\n",
    "DISGUST_BOOST = 2.0\n",
    "\n",
    "base_class_weights[angry_idx]   *= ANGRY_BOOST\n",
    "base_class_weights[disgust_idx] *= DISGUST_BOOST\n",
    "\n",
    "# Normalized to \"approximately equal to 1\"\n",
    "sum_w = sum(base_class_weights)\n",
    "class_weights = [w / sum_w * NUM_CLASSES for w in base_class_weights]\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights (for CE, after angry/disgust boost):\", class_weights_tensor.tolist())\n",
    "\n",
    "# ----- Balanced Sampler -----\n",
    "sample_weights = [class_weights[label] for label in orig_targets]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. MobileNetV2 Backbone + mouth with head\n",
    "# =========================\n",
    "class FER7WithMouth(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 backbone + primary head (full face) + auxiliary head (mouth)\n",
    "    Since MobileNetV2's final feature dimension is also 1280, the original head structure can be reused.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes=7, feat_dim=1280):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Main Head: Full Face\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Mouth\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        feat_full  = self.backbone(x_full)   # [B, 1280]\n",
    "        feat_mouth = self.backbone(x_mouth)  # [B, 1280]\n",
    "\n",
    "        logits_main  = self.head_main(feat_full)\n",
    "        logits_mouth = self.head_mouth(feat_mouth)\n",
    "        return logits_main, logits_mouth\n",
    "\n",
    "    def forward_main(self, x_full):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        return self.head_main(feat_full)\n",
    "\n",
    "print(\"Building MobileNetV2 backbone...\")\n",
    "\n",
    "if HAS_WEIGHTS_ENUM:\n",
    "    # base = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    base = mobilenet_v2(weights=None)\n",
    "else:\n",
    "    base = mobilenet_v2(weights=None)\n",
    "\n",
    "# MobileNetV2: features -> avgpool -> flatten -> classifier\n",
    "base.classifier = nn.Identity()\n",
    "\n",
    "model = FER7WithMouth(base, num_classes=NUM_CLASSES, feat_dim=1280).to(device)\n",
    "print(\"Model built with MobileNetV2 backbone.\")\n",
    "\n",
    "# with class weight + label smoothing\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=class_weights_tensor,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Weighting for the auxiliary head loss (reduced to prevent masking upper facial features like eyebrows)\n",
    "LAMBDA_MOUTH = 0.3\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. LR warmup + cosine decay\n",
    "# =========================\n",
    "total_steps  = EPOCHS * len(train_loader)\n",
    "warmup_ratio = 0.1\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return BASE_LR * float(step + 1) / float(warmup_steps + 1)\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    min_lr = BASE_LR * 0.1\n",
    "    return min_lr + (BASE_LR - min_lr) * cosine\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. Training Cycle\n",
    "# =========================\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    ce_main_sum = 0.0\n",
    "    ce_mouth_sum = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=100)\n",
    "    for imgs, labels in pbar:\n",
    "        y = torch.tensor(labels, dtype=torch.long).to(device)  # [B]\n",
    "\n",
    "        # Full Face\n",
    "        x_full = torch.stack([train_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "        # Mouth\n",
    "        mouth_imgs = [crop_mouth_pil(img) for img in imgs]\n",
    "        x_mouth = torch.stack([mouth_aug(m) for m in mouth_imgs]).to(device)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(global_step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "        # forward\n",
    "        logits_main, logits_mouth = model(x_full, x_mouth)\n",
    "\n",
    "        ce_main  = criterion(logits_main, y)\n",
    "        ce_mouth = criterion(logits_mouth, y)\n",
    "\n",
    "        loss = ce_main + LAMBDA_MOUTH * ce_mouth\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss  += loss.item()\n",
    "        ce_main_sum += ce_main.item()\n",
    "        ce_mouth_sum += ce_mouth.item()\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\":  f\"{loss.item():.4f}\",\n",
    "            \"main\":  f\"{ce_main.item():.4f}\",\n",
    "            \"mouth\": f\"{ce_mouth.item():.4f}\",\n",
    "            \"lr\":    f\"{lr:.1e}\"\n",
    "        })\n",
    "\n",
    "    avg_loss  = train_loss / len(train_loader)\n",
    "    avg_main  = ce_main_sum / len(train_loader)\n",
    "    avg_mouth = ce_mouth_sum / len(train_loader)\n",
    "    print(f\"[Train] Epoch {epoch+1} | loss={avg_loss:.4f} | main={avg_main:.4f} | mouth={avg_mouth:.4f}\")\n",
    "\n",
    "    # =========================\n",
    "    # 8. Verification (Main Header Only)\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            y_val = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            x_val = torch.stack([val_aug(img) for img in imgs]).to(device)\n",
    "\n",
    "            logits_val = model.forward_main(x_val)\n",
    "            loss_val = criterion(logits_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = logits_val.argmax(dim=1)\n",
    "            correct += (preds == y_val).sum().item()\n",
    "            total += len(y_val)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc   = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Val]   Epoch {epoch+1} | loss={val_loss:.4f} | acc={val_acc*100:.2f}%\\n\")\n",
    "\n",
    "# =========================\n",
    "# 9. 保存模型\n",
    "# =========================\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Saved MobileNetV2-based model to {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n1/34sz7g4n08jg7s8blpt80cqc0000gn/T/ipykernel_66496/1547402268.py:74: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1129 22:10:51.158000 66496 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 11).\n",
      "Failed to convert the model to the target version 11 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"/Users/zhangyizhou/miniconda3/envs/tf-env/lib/python3.10/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 156 of general pattern rewrite rules.\n",
      "Exported ONNX model to: fer7_mobilenet_mouth.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "class FER7WithMouth(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 backbone + primary head (full face) + auxiliary head (mouth)\n",
    "    Note: feat_dim=1280, consistent with the training script\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes=7, feat_dim=1280):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.head_main = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.head_mouth = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_full, x_mouth):\n",
    "        # x_full, x_mouth: [B,3,224,224]\n",
    "        feat_full  = self.backbone(x_full)   # [B,1280]\n",
    "        feat_mouth = self.backbone(x_mouth)  # [B,1280]\n",
    "\n",
    "        logits_main  = self.head_main(feat_full)\n",
    "        logits_mouth = self.head_mouth(feat_mouth)\n",
    "        return logits_main, logits_mouth\n",
    "\n",
    "    def forward_main(self, x_full):\n",
    "        feat_full = self.backbone(x_full)\n",
    "        return self.head_main(feat_full)\n",
    "\n",
    "\n",
    "def build_model(checkpoint_path, device=\"cpu\"):\n",
    "    base = mobilenet_v2(weights=None)\n",
    "    base.classifier = nn.Identity()\n",
    "\n",
    "    model = FER7WithMouth(base, num_classes=7, feat_dim=1280)\n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CHECKPOINT = \"affectnet_7cls_mouth_mobilenet.pth\"\n",
    "    ONNX_PATH  = \"fer7_mobilenet_mouth.onnx\"\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model = build_model(CHECKPOINT, device=device)\n",
    "\n",
    "    dummy_full  = torch.randn(1, 3, 224, 224, dtype=torch.float32, device=device)\n",
    "    dummy_mouth = torch.randn(1, 3, 224, 224, dtype=torch.float32, device=device)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_full, dummy_mouth), \n",
    "        ONNX_PATH,\n",
    "        input_names   = [\"full_input\", \"mouth_input\"],\n",
    "        output_names  = [\"main_logits\", \"mouth_logits\"],\n",
    "        opset_version = 11, \n",
    "        dynamic_axes  = {\n",
    "            \"full_input\":  {0: \"batch\"},\n",
    "            \"mouth_input\": {0: \"batch\"},\n",
    "            \"main_logits\": {0: \"batch\"},\n",
    "            \"mouth_logits\":{0: \"batch\"},\n",
    "        },\n",
    "        do_constant_folding = True,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    print(\"Exported ONNX model to:\", ONNX_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
